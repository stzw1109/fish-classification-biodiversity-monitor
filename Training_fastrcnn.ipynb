{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Load Pre-trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model loaded and modified for inference.\n",
      "Success: Custom model loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "model_path = \"/home/somel/code/FYP_Project/models/fast_rcnn_model.pth\"\n",
    "num_classes = 4  # Adjust based on your classes\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "print(\"Pre-trained model loaded and modified for inference.\")\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Success: Custom model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Import Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yvzqaushvDbP"
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "# Set PyTorch CUDA memory configuration\n",
    "# helps with reducing GPU memory fragmentation, therefore preventing out-of-memory errors\n",
    "os.environ['PYTORCH_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eCTMXvbE9r-T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset directories and files...\n",
      "✓ /home/somel/code/FYP_Project/Dataset/COCO/version 1/train exists\n",
      "✓ /home/somel/code/FYP_Project/Dataset/COCO/version 1/valid exists\n",
      "✓ /home/somel/code/FYP_Project/Dataset/COCO/version 1/train/_annotations.coco.json exists\n",
      "✓ /home/somel/code/FYP_Project/Dataset/COCO/version 1/valid/_annotations.coco.json exists\n",
      "Number of training images: 2925\n",
      "Number of validation images: 122\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Dataset Preparation\n",
    "# Set the root directory for your dataset\n",
    "root_dir = \"/home/somel/code/FYP_Project/Dataset/COCO/version 1\"  # Change this to your dataset path if different\n",
    "\n",
    "train_images_dir = os.path.join(root_dir, \"train\")\n",
    "train_annotations_file = os.path.join(root_dir, \"train\", \"_annotations.coco.json\")\n",
    "val_images_dir = os.path.join(root_dir, \"valid\")\n",
    "val_annotations_file = os.path.join(root_dir, \"valid\", \"_annotations.coco.json\")\n",
    "\n",
    "# Check if directories and files exist\n",
    "print(\"Checking dataset directories and files...\")\n",
    "for dir_path in [train_images_dir, val_images_dir]:\n",
    "    if os.path.exists(dir_path):\n",
    "        print(f\"✓ {dir_path} exists\")\n",
    "    else:\n",
    "        print(f\"✗ {dir_path} does not exist - please create and populate it\")\n",
    "\n",
    "for file_path in [train_annotations_file, val_annotations_file]:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"✓ {file_path} exists\")\n",
    "    else:\n",
    "        print(f\"✗ {file_path} does not exist - please export from Roboflow\")\n",
    "\n",
    "# List number of files (optional check)\n",
    "if os.path.exists(train_images_dir):\n",
    "    num_train_images = len([f for f in os.listdir(train_images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    print(f\"Number of training images: {num_train_images}\")\n",
    "if os.path.exists(val_images_dir):\n",
    "    num_val_images = len([f for f in os.listdir(val_images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    print(f\"Number of validation images: {num_val_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Augmentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8_ioEXek9xPF"
   },
   "outputs": [],
   "source": [
    "# Define Data Transforms\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "def get_transform_function(train):\n",
    "    \"\"\"\n",
    "    Define transforms for training and validation.\n",
    "    This function returns a callable that takes (image, target) and returns transformed (image, target).\n",
    "    ToTensor is applied here to convert PIL Image to PyTorch Tensor.\n",
    "    \"\"\"\n",
    "    def transform_func(image, target):\n",
    "        image = F.to_tensor(image) # Convert PIL Image to Tensor\n",
    "        # Add augmentations here if needed, they should operate on tensors and modify target accordingly\n",
    "        return image, target\n",
    "    return transform_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0ylByzx991CI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/somel/code/FYP_Project/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/somel/code/FYP_Project/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 4 classes\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "# Load and Modify the Pre-trained Fast R-CNN Model\n",
    "# Load pre-trained Fast R-CNN with ResNet-50 backbone\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Number of classes: background + number of fish classes\n",
    "# For your project, if you have multiple species (e.g., Bass, Tilapia), set accordingly\n",
    "# For now, assuming 2 fish classes + background = 3\n",
    "num_classes = 4 # Adjust based on your classes\n",
    "\n",
    "# Modify the box predictor head\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "print(f\"Model loaded with {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lCN4dfp-92zC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Training dataset size: 2925\n",
      "Validation dataset size: 122\n",
      "Batch size: 2\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "# Setting up Data Loaders\n",
    "# Create training and validation datasets using CocoDetection\n",
    "# CocoDetection(root, annFile, transforms=None) - root is images folder, annFile is annotations.json\n",
    "train_dataset = CocoDetection(root=train_images_dir, annFile=train_annotations_file, transforms=get_transform_function(train=True))\n",
    "val_dataset = CocoDetection(root=val_images_dir, annFile=val_annotations_file, transforms=get_transform_function(train=False))\n",
    "\n",
    "# Create data loaders\n",
    "# Batch size: adjust based on your GPU memory (start small, e.g., 2)\n",
    "batch_size = 2\n",
    "# num_workers: set to 0 if you encounter issues, or match your CPU cores\n",
    "num_workers = 4\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=lambda x: tuple(zip(*x))  # Custom collate for variable-sized targets\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found checkpoint at /home/somel/code/FYP_Project/Training_checkpoints/run1/last_checkpoint.pth. Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_500781/1132500680.py:23: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_500781/1132500680.py:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 0, Loss: 0.1291\n",
      "Epoch 4, Batch 10, Loss: 0.0388\n",
      "Epoch 4, Batch 20, Loss: 0.2162\n",
      "Epoch 4, Batch 30, Loss: 0.0767\n",
      "Epoch 4, Batch 40, Loss: 0.0305\n",
      "Epoch 4, Batch 50, Loss: 0.0674\n",
      "Epoch 4, Batch 60, Loss: 0.0792\n",
      "Epoch 4, Batch 70, Loss: 0.0851\n",
      "Epoch 4, Batch 80, Loss: 0.0632\n",
      "Epoch 4, Batch 90, Loss: 0.1154\n",
      "Epoch 4, Batch 100, Loss: 0.0732\n",
      "Epoch 4, Batch 110, Loss: 0.0966\n",
      "Epoch 4, Batch 120, Loss: 0.0869\n",
      "Epoch 4, Batch 130, Loss: 0.0665\n",
      "Epoch 4, Batch 140, Loss: 0.0874\n",
      "Epoch 4, Batch 150, Loss: 0.1082\n",
      "Epoch 4, Batch 160, Loss: 0.5644\n",
      "Epoch 4, Batch 170, Loss: 0.0783\n",
      "Epoch 4, Batch 180, Loss: 0.0367\n",
      "Epoch 4, Batch 190, Loss: 0.0939\n",
      "Epoch 4, Batch 200, Loss: 0.0791\n",
      "Epoch 4, Batch 210, Loss: 0.1020\n",
      "Epoch 4, Batch 220, Loss: 0.0941\n",
      "Epoch 4, Batch 230, Loss: 0.0532\n",
      "Epoch 4, Batch 240, Loss: 0.0565\n",
      "Epoch 4, Batch 250, Loss: 0.5254\n",
      "Epoch 4, Batch 260, Loss: 0.0475\n",
      "Epoch 4, Batch 270, Loss: 0.0446\n",
      "Epoch 4, Batch 280, Loss: 0.0307\n",
      "Epoch 4, Batch 290, Loss: 0.0583\n",
      "Epoch 4, Batch 300, Loss: 0.0866\n",
      "Epoch 4, Batch 310, Loss: 0.0505\n",
      "Epoch 4, Batch 320, Loss: 0.1071\n",
      "Epoch 4, Batch 330, Loss: 0.0538\n",
      "Epoch 4, Batch 340, Loss: 0.0668\n",
      "Epoch 4, Batch 350, Loss: 0.0505\n",
      "Epoch 4, Batch 360, Loss: 0.0556\n",
      "Epoch 4, Batch 370, Loss: 0.0693\n",
      "Epoch 4, Batch 380, Loss: 0.1194\n",
      "Epoch 4, Batch 390, Loss: 0.0598\n",
      "Epoch 4, Batch 400, Loss: 0.0644\n",
      "Epoch 4, Batch 410, Loss: 0.0436\n",
      "Epoch 4, Batch 420, Loss: 0.0743\n",
      "Epoch 4, Batch 430, Loss: 0.0464\n",
      "Epoch 4, Batch 440, Loss: 0.0637\n",
      "Epoch 4, Batch 450, Loss: 0.1476\n",
      "Epoch 4, Batch 460, Loss: 0.0985\n",
      "Epoch 4, Batch 470, Loss: 0.3686\n",
      "Epoch 4, Batch 480, Loss: 0.0369\n",
      "Epoch 4, Batch 490, Loss: 0.4624\n",
      "Epoch 4, Batch 500, Loss: 0.0475\n",
      "Epoch 4, Batch 510, Loss: 0.0777\n",
      "Epoch 4, Batch 520, Loss: 0.0513\n",
      "Epoch 4, Batch 530, Loss: 0.0637\n",
      "Epoch 4, Batch 540, Loss: 0.0460\n",
      "Epoch 4, Batch 550, Loss: 0.0419\n",
      "Epoch 4, Batch 560, Loss: 0.0633\n",
      "Epoch 4, Batch 570, Loss: 0.1253\n",
      "Epoch 4, Batch 580, Loss: 0.1445\n",
      "Epoch 4, Batch 590, Loss: 0.0488\n",
      "Epoch 4, Batch 600, Loss: 0.0356\n",
      "Epoch 4, Batch 610, Loss: 0.0657\n",
      "Epoch 4, Batch 620, Loss: 0.0853\n",
      "Epoch 4, Batch 630, Loss: 0.0552\n",
      "Epoch 4, Batch 640, Loss: 0.0837\n",
      "Epoch 4, Batch 650, Loss: 0.0633\n",
      "Epoch 4, Batch 660, Loss: 0.0840\n",
      "Epoch 4, Batch 670, Loss: 0.0530\n",
      "Epoch 4, Batch 680, Loss: 0.0812\n",
      "Epoch 4, Batch 690, Loss: 0.0784\n",
      "Epoch 4, Batch 700, Loss: 0.0565\n",
      "Epoch 4, Batch 710, Loss: 0.0638\n",
      "Epoch 4, Batch 720, Loss: 0.0662\n",
      "Epoch 4, Batch 730, Loss: 0.1075\n",
      "Epoch 4, Batch 740, Loss: 0.0573\n",
      "Epoch 4, Batch 750, Loss: 0.0883\n",
      "Epoch 4, Batch 760, Loss: 0.0364\n",
      "Epoch 4, Batch 770, Loss: 0.5739\n",
      "Epoch 4, Batch 780, Loss: 0.0975\n",
      "Epoch 4, Batch 790, Loss: 0.1480\n",
      "Epoch 4, Batch 800, Loss: 0.0903\n",
      "Epoch 4, Batch 810, Loss: 0.0756\n",
      "Epoch 4, Batch 820, Loss: 0.0839\n",
      "Epoch 4, Batch 830, Loss: 0.0910\n",
      "Epoch 4, Batch 840, Loss: 0.0488\n",
      "Epoch 4, Batch 850, Loss: 0.0556\n",
      "Epoch 4, Batch 860, Loss: 0.3184\n",
      "Epoch 4, Batch 870, Loss: 0.0605\n",
      "Epoch 4, Batch 880, Loss: 0.0564\n",
      "Epoch 4, Batch 890, Loss: 0.0515\n",
      "Epoch 4, Batch 900, Loss: 0.0525\n",
      "Epoch 4, Batch 910, Loss: 0.0383\n",
      "Epoch 4, Batch 920, Loss: 0.0650\n",
      "Epoch 4, Batch 930, Loss: 0.0903\n",
      "Epoch 4, Batch 940, Loss: 0.0538\n",
      "Epoch 4, Batch 950, Loss: 0.1984\n",
      "Epoch 4, Batch 960, Loss: 0.0567\n",
      "Epoch 4, Batch 970, Loss: 0.0624\n",
      "Epoch 4, Batch 980, Loss: 0.0522\n",
      "Epoch 4, Batch 990, Loss: 0.1789\n",
      "Epoch 4, Batch 1000, Loss: 0.0610\n",
      "Epoch 4, Batch 1010, Loss: 0.0991\n",
      "Epoch 4, Batch 1020, Loss: 0.0809\n",
      "Epoch 4, Batch 1030, Loss: 0.0597\n",
      "Epoch 4, Batch 1040, Loss: 0.0607\n",
      "Epoch 4, Batch 1050, Loss: 0.0696\n",
      "Epoch 4, Batch 1060, Loss: 0.1171\n",
      "Epoch 4, Batch 1070, Loss: 0.1176\n",
      "Epoch 4, Batch 1080, Loss: 0.0587\n",
      "Epoch 4, Batch 1090, Loss: 0.1197\n",
      "Epoch 4, Batch 1100, Loss: 0.0974\n",
      "Epoch 4, Batch 1110, Loss: 0.0831\n",
      "Epoch 4, Batch 1120, Loss: 0.0890\n",
      "Epoch 4, Batch 1130, Loss: 0.6700\n",
      "Epoch 4, Batch 1140, Loss: 0.2487\n",
      "Epoch 4, Batch 1150, Loss: 0.0624\n",
      "Epoch 4, Batch 1160, Loss: 0.0644\n",
      "Epoch 4, Batch 1170, Loss: 0.1264\n",
      "Epoch 4, Batch 1180, Loss: 0.1118\n",
      "Epoch 4, Batch 1190, Loss: 0.0773\n",
      "Epoch 4, Batch 1200, Loss: 0.0631\n",
      "Epoch 4, Batch 1210, Loss: 0.1142\n",
      "Epoch 4, Batch 1220, Loss: 0.0684\n",
      "Epoch 4, Batch 1230, Loss: 0.1979\n",
      "Epoch 4, Batch 1240, Loss: 0.0913\n",
      "Epoch 4, Batch 1250, Loss: 0.0626\n",
      "Epoch 4, Batch 1260, Loss: 0.0827\n",
      "Epoch 4, Batch 1270, Loss: 0.2719\n",
      "Epoch 4, Batch 1280, Loss: 0.0950\n",
      "Epoch 4, Batch 1290, Loss: 0.0727\n",
      "Epoch 4, Batch 1300, Loss: 0.0547\n",
      "Epoch 4, Batch 1310, Loss: 0.0483\n",
      "Epoch 4, Batch 1320, Loss: 0.0793\n",
      "Epoch 4, Batch 1330, Loss: 0.1217\n",
      "Epoch 4, Batch 1340, Loss: 0.0472\n",
      "Epoch 4, Batch 1350, Loss: 0.0543\n",
      "Epoch 4, Batch 1360, Loss: 0.0871\n",
      "Epoch 4, Batch 1370, Loss: 0.0664\n",
      "Epoch 4, Batch 1380, Loss: 0.1406\n",
      "Epoch 4, Batch 1390, Loss: 0.0525\n",
      "Epoch 4, Batch 1400, Loss: 0.0755\n",
      "Epoch 4, Batch 1410, Loss: 0.1187\n",
      "Epoch 4, Batch 1420, Loss: 0.2307\n",
      "Epoch 4, Batch 1430, Loss: 0.0468\n",
      "Epoch 4, Batch 1440, Loss: 0.1402\n",
      "Epoch 4, Batch 1450, Loss: 0.0956\n",
      "Epoch 4, Batch 1460, Loss: 0.0829\n",
      "\n",
      "Epoch 4/50, Average Loss: 0.1014\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved for Epoch 4\n",
      "\n",
      "\n",
      "Epoch 5, Batch 0, Loss: 0.2466\n",
      "Epoch 5, Batch 10, Loss: 0.0548\n",
      "Epoch 5, Batch 20, Loss: 0.0887\n",
      "Epoch 5, Batch 30, Loss: 0.0587\n",
      "Epoch 5, Batch 40, Loss: 0.0709\n",
      "Epoch 5, Batch 50, Loss: 0.0699\n",
      "Epoch 5, Batch 60, Loss: 0.0381\n",
      "Epoch 5, Batch 70, Loss: 0.0266\n",
      "Epoch 5, Batch 80, Loss: 0.0674\n",
      "Epoch 5, Batch 90, Loss: 0.0743\n",
      "Epoch 5, Batch 100, Loss: 0.1133\n",
      "Epoch 5, Batch 110, Loss: 0.0939\n",
      "Epoch 5, Batch 120, Loss: 0.0518\n",
      "Epoch 5, Batch 130, Loss: 0.0692\n",
      "Epoch 5, Batch 140, Loss: 0.0598\n",
      "Epoch 5, Batch 150, Loss: 0.6151\n",
      "Epoch 5, Batch 160, Loss: 0.0543\n",
      "Epoch 5, Batch 170, Loss: 0.0384\n",
      "Epoch 5, Batch 180, Loss: 0.0518\n",
      "Epoch 5, Batch 190, Loss: 0.0681\n",
      "Epoch 5, Batch 200, Loss: 0.0519\n",
      "Epoch 5, Batch 210, Loss: 0.0404\n",
      "Epoch 5, Batch 220, Loss: 0.0513\n",
      "Epoch 5, Batch 230, Loss: 0.0616\n",
      "Epoch 5, Batch 240, Loss: 0.1102\n",
      "Epoch 5, Batch 250, Loss: 0.0574\n",
      "Epoch 5, Batch 260, Loss: 0.0492\n",
      "Epoch 5, Batch 270, Loss: 0.0978\n",
      "Epoch 5, Batch 280, Loss: 0.0553\n",
      "Epoch 5, Batch 290, Loss: 0.0356\n",
      "Epoch 5, Batch 300, Loss: 0.0890\n",
      "Epoch 5, Batch 310, Loss: 0.0615\n",
      "Epoch 5, Batch 320, Loss: 0.0648\n",
      "Epoch 5, Batch 330, Loss: 0.0622\n",
      "Epoch 5, Batch 340, Loss: 0.0673\n",
      "Epoch 5, Batch 350, Loss: 0.0495\n",
      "Epoch 5, Batch 360, Loss: 0.0547\n",
      "Epoch 5, Batch 370, Loss: 0.0556\n",
      "Epoch 5, Batch 380, Loss: 0.0797\n",
      "Epoch 5, Batch 390, Loss: 0.0574\n",
      "Epoch 5, Batch 400, Loss: 0.0610\n",
      "Epoch 5, Batch 410, Loss: 0.0531\n",
      "Epoch 5, Batch 420, Loss: 0.0699\n",
      "Epoch 5, Batch 430, Loss: 0.0568\n",
      "Epoch 5, Batch 440, Loss: 0.0506\n",
      "Epoch 5, Batch 450, Loss: 0.2900\n",
      "Epoch 5, Batch 460, Loss: 0.0866\n",
      "Epoch 5, Batch 470, Loss: 0.0485\n",
      "Epoch 5, Batch 480, Loss: 0.0275\n",
      "Epoch 5, Batch 490, Loss: 0.4833\n",
      "Epoch 5, Batch 500, Loss: 0.0769\n",
      "Epoch 5, Batch 510, Loss: 0.0688\n",
      "Epoch 5, Batch 520, Loss: 0.0549\n",
      "Epoch 5, Batch 530, Loss: 0.0492\n",
      "Epoch 5, Batch 540, Loss: 0.0978\n",
      "Epoch 5, Batch 550, Loss: 0.0851\n",
      "Epoch 5, Batch 560, Loss: 0.0536\n",
      "Epoch 5, Batch 570, Loss: 0.1217\n",
      "Epoch 5, Batch 580, Loss: 0.3829\n",
      "Epoch 5, Batch 590, Loss: 0.1634\n",
      "Epoch 5, Batch 600, Loss: 0.2903\n",
      "Epoch 5, Batch 610, Loss: 0.0460\n",
      "Epoch 5, Batch 620, Loss: 0.0514\n",
      "Epoch 5, Batch 630, Loss: 0.3803\n",
      "Epoch 5, Batch 640, Loss: 0.1219\n",
      "Epoch 5, Batch 650, Loss: 0.0436\n",
      "Epoch 5, Batch 660, Loss: 0.0473\n",
      "Epoch 5, Batch 670, Loss: 0.1521\n",
      "Epoch 5, Batch 680, Loss: 0.0625\n",
      "Epoch 5, Batch 690, Loss: 0.0689\n",
      "Epoch 5, Batch 700, Loss: 0.1188\n",
      "Epoch 5, Batch 710, Loss: 0.0836\n",
      "Epoch 5, Batch 720, Loss: 0.0943\n",
      "Epoch 5, Batch 730, Loss: 0.0603\n",
      "Epoch 5, Batch 740, Loss: 0.1095\n",
      "Epoch 5, Batch 750, Loss: 0.0682\n",
      "Epoch 5, Batch 760, Loss: 0.0434\n",
      "Epoch 5, Batch 770, Loss: 0.0804\n",
      "Epoch 5, Batch 780, Loss: 0.2205\n",
      "Epoch 5, Batch 790, Loss: 0.1162\n",
      "Epoch 5, Batch 800, Loss: 0.0559\n",
      "Epoch 5, Batch 810, Loss: 0.0837\n",
      "Epoch 5, Batch 820, Loss: 0.0436\n",
      "Epoch 5, Batch 830, Loss: 0.0364\n",
      "Epoch 5, Batch 840, Loss: 0.0724\n",
      "Epoch 5, Batch 850, Loss: 0.0723\n",
      "Epoch 5, Batch 860, Loss: 0.0429\n",
      "Epoch 5, Batch 870, Loss: 0.0561\n",
      "Epoch 5, Batch 880, Loss: 0.4972\n",
      "Epoch 5, Batch 890, Loss: 0.0871\n",
      "Epoch 5, Batch 900, Loss: 0.0542\n",
      "Epoch 5, Batch 910, Loss: 0.0514\n",
      "Epoch 5, Batch 920, Loss: 0.0621\n",
      "Epoch 5, Batch 930, Loss: 0.0451\n",
      "Epoch 5, Batch 940, Loss: 0.0798\n",
      "Epoch 5, Batch 950, Loss: 0.0517\n",
      "Epoch 5, Batch 960, Loss: 0.0879\n",
      "Epoch 5, Batch 970, Loss: 0.0625\n",
      "Epoch 5, Batch 980, Loss: 0.0685\n",
      "Epoch 5, Batch 990, Loss: 0.0809\n",
      "Epoch 5, Batch 1000, Loss: 0.0416\n",
      "Epoch 5, Batch 1010, Loss: 0.2579\n",
      "Epoch 5, Batch 1020, Loss: 0.0583\n",
      "Epoch 5, Batch 1030, Loss: 0.1677\n",
      "Epoch 5, Batch 1040, Loss: 0.3693\n",
      "Epoch 5, Batch 1050, Loss: 0.2290\n",
      "Epoch 5, Batch 1060, Loss: 0.0506\n",
      "Epoch 5, Batch 1070, Loss: 0.0639\n",
      "Epoch 5, Batch 1080, Loss: 0.2509\n",
      "Epoch 5, Batch 1090, Loss: 0.0799\n",
      "Epoch 5, Batch 1100, Loss: 0.0443\n",
      "Epoch 5, Batch 1110, Loss: 0.0884\n",
      "Epoch 5, Batch 1120, Loss: 0.1413\n",
      "Epoch 5, Batch 1130, Loss: 0.0918\n",
      "Epoch 5, Batch 1140, Loss: 0.1147\n",
      "Epoch 5, Batch 1150, Loss: 0.1431\n",
      "Epoch 5, Batch 1160, Loss: 0.0660\n",
      "Epoch 5, Batch 1170, Loss: 0.0505\n",
      "Epoch 5, Batch 1180, Loss: 0.1155\n",
      "Epoch 5, Batch 1190, Loss: 0.0414\n",
      "Epoch 5, Batch 1200, Loss: 0.3079\n",
      "Epoch 5, Batch 1210, Loss: 0.0857\n",
      "Epoch 5, Batch 1220, Loss: 0.0768\n",
      "Epoch 5, Batch 1230, Loss: 0.0365\n",
      "Epoch 5, Batch 1240, Loss: 0.0819\n",
      "Epoch 5, Batch 1250, Loss: 0.1722\n",
      "Epoch 5, Batch 1260, Loss: 0.0509\n",
      "Epoch 5, Batch 1270, Loss: 0.1487\n",
      "Epoch 5, Batch 1280, Loss: 0.0532\n",
      "Epoch 5, Batch 1290, Loss: 0.0840\n",
      "Epoch 5, Batch 1300, Loss: 0.0684\n",
      "Epoch 5, Batch 1310, Loss: 0.0693\n",
      "Epoch 5, Batch 1320, Loss: 0.1873\n",
      "Epoch 5, Batch 1330, Loss: 0.0362\n",
      "Epoch 5, Batch 1340, Loss: 0.1320\n",
      "Epoch 5, Batch 1350, Loss: 0.0506\n",
      "Epoch 5, Batch 1360, Loss: 0.0587\n",
      "Epoch 5, Batch 1370, Loss: 0.0683\n",
      "Epoch 5, Batch 1380, Loss: 0.0671\n",
      "Epoch 5, Batch 1390, Loss: 0.0624\n",
      "Epoch 5, Batch 1400, Loss: 0.0680\n",
      "Epoch 5, Batch 1410, Loss: 0.0528\n",
      "Epoch 5, Batch 1420, Loss: 0.0619\n",
      "Epoch 5, Batch 1430, Loss: 0.0871\n",
      "Epoch 5, Batch 1440, Loss: 0.0851\n",
      "Epoch 5, Batch 1450, Loss: 0.1327\n",
      "Epoch 5, Batch 1460, Loss: 0.0647\n",
      "\n",
      "Epoch 5/50, Average Loss: 0.0962\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved for Epoch 5\n",
      "\n",
      "\n",
      "Epoch 6, Batch 0, Loss: 0.2768\n",
      "Epoch 6, Batch 10, Loss: 0.1248\n",
      "Epoch 6, Batch 20, Loss: 0.0495\n",
      "Epoch 6, Batch 30, Loss: 0.1022\n",
      "Epoch 6, Batch 40, Loss: 0.0558\n",
      "Epoch 6, Batch 50, Loss: 0.0388\n",
      "Epoch 6, Batch 60, Loss: 0.0803\n",
      "Epoch 6, Batch 70, Loss: 0.0850\n",
      "Epoch 6, Batch 80, Loss: 0.1515\n",
      "Epoch 6, Batch 90, Loss: 0.0504\n",
      "Epoch 6, Batch 100, Loss: 0.0619\n",
      "Epoch 6, Batch 110, Loss: 0.1129\n",
      "Epoch 6, Batch 120, Loss: 0.0550\n",
      "Epoch 6, Batch 130, Loss: 0.0785\n",
      "Epoch 6, Batch 140, Loss: 0.0566\n",
      "Epoch 6, Batch 150, Loss: 0.1754\n",
      "Epoch 6, Batch 160, Loss: 0.0517\n",
      "Epoch 6, Batch 170, Loss: 0.0820\n",
      "Epoch 6, Batch 180, Loss: 0.0520\n",
      "Epoch 6, Batch 190, Loss: 0.1225\n",
      "Epoch 6, Batch 200, Loss: 0.1070\n",
      "Epoch 6, Batch 210, Loss: 0.0325\n",
      "Epoch 6, Batch 220, Loss: 0.0536\n",
      "Epoch 6, Batch 230, Loss: 0.0489\n",
      "Epoch 6, Batch 240, Loss: 0.0503\n",
      "Epoch 6, Batch 250, Loss: 0.2787\n",
      "Epoch 6, Batch 260, Loss: 0.0573\n",
      "Epoch 6, Batch 270, Loss: 0.1203\n",
      "Epoch 6, Batch 280, Loss: 0.0481\n",
      "Epoch 6, Batch 290, Loss: 0.0677\n",
      "Epoch 6, Batch 300, Loss: 0.1174\n",
      "Epoch 6, Batch 310, Loss: 0.0452\n",
      "Epoch 6, Batch 320, Loss: 0.3438\n",
      "Epoch 6, Batch 330, Loss: 0.1841\n",
      "Epoch 6, Batch 340, Loss: 0.0401\n",
      "Epoch 6, Batch 350, Loss: 0.0895\n",
      "Epoch 6, Batch 360, Loss: 0.0491\n",
      "Epoch 6, Batch 370, Loss: 0.0421\n",
      "Epoch 6, Batch 380, Loss: 0.0550\n",
      "Epoch 6, Batch 390, Loss: 0.0760\n",
      "Epoch 6, Batch 400, Loss: 0.3744\n",
      "Epoch 6, Batch 410, Loss: 0.0988\n",
      "Epoch 6, Batch 420, Loss: 0.0754\n",
      "Epoch 6, Batch 430, Loss: 0.0570\n",
      "Epoch 6, Batch 440, Loss: 0.0514\n",
      "Epoch 6, Batch 450, Loss: 0.0608\n",
      "Epoch 6, Batch 460, Loss: 0.0484\n",
      "Epoch 6, Batch 470, Loss: 0.0491\n",
      "Epoch 6, Batch 480, Loss: 0.0692\n",
      "Epoch 6, Batch 490, Loss: 0.1100\n",
      "Epoch 6, Batch 500, Loss: 0.0772\n",
      "Epoch 6, Batch 510, Loss: 0.0677\n",
      "Epoch 6, Batch 520, Loss: 0.0435\n",
      "Epoch 6, Batch 530, Loss: 0.0804\n",
      "Epoch 6, Batch 540, Loss: 0.0503\n",
      "Epoch 6, Batch 550, Loss: 0.3463\n",
      "Epoch 6, Batch 560, Loss: 0.2365\n",
      "Epoch 6, Batch 570, Loss: 0.0385\n",
      "Epoch 6, Batch 580, Loss: 0.2213\n",
      "Epoch 6, Batch 590, Loss: 0.0541\n",
      "Epoch 6, Batch 600, Loss: 0.0882\n",
      "Epoch 6, Batch 610, Loss: 0.0900\n",
      "Epoch 6, Batch 620, Loss: 0.0395\n",
      "Epoch 6, Batch 630, Loss: 0.0776\n",
      "Epoch 6, Batch 640, Loss: 0.0494\n",
      "Epoch 6, Batch 650, Loss: 0.0359\n",
      "Epoch 6, Batch 660, Loss: 0.1241\n",
      "Epoch 6, Batch 670, Loss: 0.0932\n",
      "Epoch 6, Batch 680, Loss: 0.0623\n",
      "Epoch 6, Batch 690, Loss: 0.0362\n",
      "Epoch 6, Batch 700, Loss: 0.0548\n",
      "Epoch 6, Batch 710, Loss: 0.0597\n",
      "Epoch 6, Batch 720, Loss: 0.0646\n",
      "Epoch 6, Batch 730, Loss: 0.0633\n",
      "Epoch 6, Batch 740, Loss: 0.0764\n",
      "Epoch 6, Batch 750, Loss: 0.0505\n",
      "Epoch 6, Batch 760, Loss: 0.0644\n",
      "Epoch 6, Batch 770, Loss: 0.2280\n",
      "Epoch 6, Batch 780, Loss: 0.1918\n",
      "Epoch 6, Batch 790, Loss: 0.0326\n",
      "Epoch 6, Batch 800, Loss: 0.0543\n",
      "Epoch 6, Batch 810, Loss: 0.1769\n",
      "Epoch 6, Batch 820, Loss: 0.1158\n",
      "Epoch 6, Batch 830, Loss: 0.0726\n",
      "Epoch 6, Batch 840, Loss: 0.0935\n",
      "Epoch 6, Batch 850, Loss: 0.0534\n",
      "Epoch 6, Batch 860, Loss: 0.0836\n",
      "Epoch 6, Batch 870, Loss: 0.0351\n",
      "Epoch 6, Batch 880, Loss: 0.0436\n",
      "Epoch 6, Batch 890, Loss: 0.0378\n",
      "Epoch 6, Batch 900, Loss: 0.0501\n",
      "Epoch 6, Batch 910, Loss: 0.0485\n",
      "Epoch 6, Batch 920, Loss: 0.0730\n",
      "Epoch 6, Batch 930, Loss: 0.0501\n",
      "Epoch 6, Batch 940, Loss: 0.0528\n",
      "Epoch 6, Batch 950, Loss: 0.0577\n",
      "Epoch 6, Batch 960, Loss: 0.0969\n",
      "Epoch 6, Batch 970, Loss: 0.0876\n",
      "Epoch 6, Batch 980, Loss: 0.0469\n",
      "Epoch 6, Batch 990, Loss: 0.0644\n",
      "Epoch 6, Batch 1000, Loss: 0.1692\n",
      "Epoch 6, Batch 1010, Loss: 0.0559\n",
      "Epoch 6, Batch 1020, Loss: 0.2823\n",
      "Epoch 6, Batch 1030, Loss: 0.0646\n",
      "Epoch 6, Batch 1040, Loss: 0.0467\n",
      "Epoch 6, Batch 1050, Loss: 0.1249\n",
      "Epoch 6, Batch 1060, Loss: 0.0429\n",
      "Epoch 6, Batch 1070, Loss: 0.0976\n",
      "Epoch 6, Batch 1080, Loss: 0.0303\n",
      "Epoch 6, Batch 1090, Loss: 0.1445\n",
      "Epoch 6, Batch 1100, Loss: 0.0712\n",
      "Epoch 6, Batch 1110, Loss: 0.0442\n",
      "Epoch 6, Batch 1120, Loss: 0.0641\n",
      "Epoch 6, Batch 1130, Loss: 0.0363\n",
      "Epoch 6, Batch 1140, Loss: 0.0839\n",
      "Epoch 6, Batch 1150, Loss: 0.0512\n",
      "Epoch 6, Batch 1160, Loss: 0.0807\n",
      "Epoch 6, Batch 1170, Loss: 0.0585\n",
      "Epoch 6, Batch 1180, Loss: 0.1014\n",
      "Epoch 6, Batch 1190, Loss: 0.2199\n",
      "Epoch 6, Batch 1200, Loss: 0.7120\n",
      "Epoch 6, Batch 1210, Loss: 0.0651\n",
      "Epoch 6, Batch 1220, Loss: 0.0992\n",
      "Epoch 6, Batch 1230, Loss: 0.1568\n",
      "Epoch 6, Batch 1240, Loss: 0.0709\n",
      "Epoch 6, Batch 1250, Loss: 0.0809\n",
      "Epoch 6, Batch 1260, Loss: 0.1145\n",
      "Epoch 6, Batch 1270, Loss: 0.0360\n",
      "Epoch 6, Batch 1280, Loss: 0.2972\n",
      "Epoch 6, Batch 1290, Loss: 0.0729\n",
      "Epoch 6, Batch 1300, Loss: 0.0593\n",
      "Epoch 6, Batch 1310, Loss: 0.2205\n",
      "Epoch 6, Batch 1320, Loss: 0.0448\n",
      "Epoch 6, Batch 1330, Loss: 0.1026\n",
      "Epoch 6, Batch 1340, Loss: 0.1512\n",
      "Epoch 6, Batch 1350, Loss: 0.0478\n",
      "Epoch 6, Batch 1360, Loss: 0.0387\n",
      "Epoch 6, Batch 1370, Loss: 0.0615\n",
      "Epoch 6, Batch 1380, Loss: 0.0646\n",
      "Epoch 6, Batch 1390, Loss: 0.0617\n",
      "Epoch 6, Batch 1400, Loss: 0.0819\n",
      "Epoch 6, Batch 1410, Loss: 0.0939\n",
      "Epoch 6, Batch 1420, Loss: 0.3387\n",
      "Epoch 6, Batch 1430, Loss: 0.0633\n",
      "Epoch 6, Batch 1440, Loss: 0.1028\n",
      "Epoch 6, Batch 1450, Loss: 0.0365\n",
      "Epoch 6, Batch 1460, Loss: 0.0666\n",
      "\n",
      "Epoch 6/50, Average Loss: 0.0947\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved for Epoch 6\n",
      "\n",
      "\n",
      "Epoch 7, Batch 0, Loss: 0.1762\n",
      "Epoch 7, Batch 10, Loss: 0.2678\n",
      "Epoch 7, Batch 20, Loss: 0.0673\n",
      "Epoch 7, Batch 30, Loss: 0.1114\n",
      "Epoch 7, Batch 40, Loss: 0.0517\n",
      "Epoch 7, Batch 50, Loss: 0.1015\n",
      "Epoch 7, Batch 60, Loss: 0.0428\n",
      "Epoch 7, Batch 70, Loss: 0.0299\n",
      "Epoch 7, Batch 80, Loss: 0.0397\n",
      "Epoch 7, Batch 90, Loss: 0.0622\n",
      "Epoch 7, Batch 100, Loss: 0.0944\n",
      "Epoch 7, Batch 110, Loss: 0.0526\n",
      "Epoch 7, Batch 120, Loss: 0.1129\n",
      "Epoch 7, Batch 130, Loss: 0.1100\n",
      "Epoch 7, Batch 140, Loss: 0.0626\n",
      "Epoch 7, Batch 150, Loss: 0.0696\n",
      "Epoch 7, Batch 160, Loss: 0.0808\n",
      "Epoch 7, Batch 170, Loss: 0.0506\n",
      "Epoch 7, Batch 180, Loss: 0.0849\n",
      "Epoch 7, Batch 190, Loss: 0.0428\n",
      "Epoch 7, Batch 200, Loss: 0.0573\n",
      "Epoch 7, Batch 210, Loss: 0.0464\n",
      "Epoch 7, Batch 220, Loss: 0.0732\n",
      "Epoch 7, Batch 230, Loss: 0.0958\n",
      "Epoch 7, Batch 240, Loss: 0.1213\n",
      "Epoch 7, Batch 250, Loss: 0.0898\n",
      "Epoch 7, Batch 260, Loss: 0.0315\n",
      "Epoch 7, Batch 270, Loss: 0.0476\n",
      "Epoch 7, Batch 280, Loss: 0.5238\n",
      "Epoch 7, Batch 290, Loss: 0.1094\n",
      "Epoch 7, Batch 300, Loss: 0.1229\n",
      "Epoch 7, Batch 310, Loss: 0.0617\n",
      "Epoch 7, Batch 320, Loss: 0.1085\n",
      "Epoch 7, Batch 330, Loss: 0.0755\n",
      "Epoch 7, Batch 340, Loss: 0.0477\n",
      "Epoch 7, Batch 350, Loss: 0.2447\n",
      "Epoch 7, Batch 360, Loss: 0.1048\n",
      "Epoch 7, Batch 370, Loss: 0.2490\n",
      "Epoch 7, Batch 380, Loss: 0.4621\n",
      "Epoch 7, Batch 390, Loss: 0.1448\n",
      "Epoch 7, Batch 400, Loss: 0.0597\n",
      "Epoch 7, Batch 410, Loss: 0.0364\n",
      "Epoch 7, Batch 420, Loss: 0.0336\n",
      "Epoch 7, Batch 430, Loss: 0.1337\n",
      "Epoch 7, Batch 440, Loss: 0.1031\n",
      "Epoch 7, Batch 450, Loss: 0.0516\n",
      "Epoch 7, Batch 460, Loss: 0.0511\n",
      "Epoch 7, Batch 470, Loss: 0.0816\n",
      "Epoch 7, Batch 480, Loss: 0.0849\n",
      "Epoch 7, Batch 490, Loss: 0.0910\n",
      "Epoch 7, Batch 500, Loss: 0.1775\n",
      "Epoch 7, Batch 510, Loss: 0.0448\n",
      "Epoch 7, Batch 520, Loss: 0.0921\n",
      "Epoch 7, Batch 530, Loss: 0.0414\n",
      "Epoch 7, Batch 540, Loss: 0.0662\n",
      "Epoch 7, Batch 550, Loss: 0.0961\n",
      "Epoch 7, Batch 560, Loss: 0.0539\n",
      "Epoch 7, Batch 570, Loss: 0.0556\n",
      "Epoch 7, Batch 580, Loss: 0.0716\n",
      "Epoch 7, Batch 590, Loss: 0.1049\n",
      "Epoch 7, Batch 600, Loss: 0.0393\n",
      "Epoch 7, Batch 610, Loss: 0.0779\n",
      "Epoch 7, Batch 620, Loss: 0.0608\n",
      "Epoch 7, Batch 630, Loss: 0.0986\n",
      "Epoch 7, Batch 640, Loss: 0.0975\n",
      "Epoch 7, Batch 650, Loss: 0.1193\n",
      "Epoch 7, Batch 660, Loss: 0.0814\n",
      "Epoch 7, Batch 670, Loss: 0.1298\n",
      "Epoch 7, Batch 680, Loss: 0.0965\n",
      "Epoch 7, Batch 690, Loss: 0.0767\n",
      "Epoch 7, Batch 700, Loss: 0.1031\n",
      "Epoch 7, Batch 710, Loss: 0.4448\n",
      "Epoch 7, Batch 720, Loss: 0.0487\n",
      "Epoch 7, Batch 730, Loss: 0.1200\n",
      "Epoch 7, Batch 740, Loss: 0.0849\n",
      "Epoch 7, Batch 750, Loss: 0.0346\n",
      "Epoch 7, Batch 760, Loss: 0.1771\n",
      "Epoch 7, Batch 770, Loss: 0.0569\n",
      "Epoch 7, Batch 780, Loss: 0.0752\n",
      "Epoch 7, Batch 790, Loss: 0.0470\n",
      "Epoch 7, Batch 800, Loss: 0.0883\n",
      "Epoch 7, Batch 810, Loss: 0.1305\n",
      "Epoch 7, Batch 820, Loss: 0.0670\n",
      "Epoch 7, Batch 830, Loss: 0.0614\n",
      "Epoch 7, Batch 840, Loss: 0.0799\n",
      "Epoch 7, Batch 850, Loss: 0.0556\n",
      "Epoch 7, Batch 860, Loss: 0.0590\n",
      "Epoch 7, Batch 870, Loss: 0.0421\n",
      "Epoch 7, Batch 880, Loss: 0.0749\n",
      "Epoch 7, Batch 890, Loss: 0.1349\n",
      "Epoch 7, Batch 900, Loss: 0.0624\n",
      "Epoch 7, Batch 910, Loss: 0.1149\n",
      "Epoch 7, Batch 920, Loss: 0.0671\n",
      "Epoch 7, Batch 930, Loss: 0.3175\n",
      "Epoch 7, Batch 940, Loss: 0.0600\n",
      "Epoch 7, Batch 950, Loss: 0.1213\n",
      "Epoch 7, Batch 960, Loss: 0.0387\n",
      "Epoch 7, Batch 970, Loss: 0.0497\n",
      "Epoch 7, Batch 980, Loss: 0.0604\n",
      "Epoch 7, Batch 990, Loss: 0.0510\n",
      "Epoch 7, Batch 1000, Loss: 0.1080\n",
      "Epoch 7, Batch 1010, Loss: 0.0536\n",
      "Epoch 7, Batch 1020, Loss: 0.0541\n",
      "Epoch 7, Batch 1030, Loss: 0.0855\n",
      "Epoch 7, Batch 1040, Loss: 0.0505\n",
      "Epoch 7, Batch 1050, Loss: 0.0915\n",
      "Epoch 7, Batch 1060, Loss: 0.0639\n",
      "Epoch 7, Batch 1070, Loss: 0.0355\n",
      "Epoch 7, Batch 1080, Loss: 0.0752\n",
      "Epoch 7, Batch 1090, Loss: 0.0889\n",
      "Epoch 7, Batch 1100, Loss: 0.2142\n",
      "Epoch 7, Batch 1110, Loss: 0.0505\n",
      "Epoch 7, Batch 1120, Loss: 0.0517\n",
      "Epoch 7, Batch 1130, Loss: 0.0899\n",
      "Epoch 7, Batch 1140, Loss: 0.0666\n",
      "Epoch 7, Batch 1150, Loss: 0.0370\n",
      "Epoch 7, Batch 1160, Loss: 0.0461\n",
      "Epoch 7, Batch 1170, Loss: 0.0851\n",
      "Epoch 7, Batch 1180, Loss: 0.0893\n",
      "Epoch 7, Batch 1190, Loss: 0.0345\n",
      "Epoch 7, Batch 1200, Loss: 0.1009\n",
      "Epoch 7, Batch 1210, Loss: 0.3706\n",
      "Epoch 7, Batch 1220, Loss: 0.0363\n",
      "Epoch 7, Batch 1230, Loss: 0.0455\n",
      "Epoch 7, Batch 1240, Loss: 0.0373\n",
      "Epoch 7, Batch 1250, Loss: 0.2288\n",
      "Epoch 7, Batch 1260, Loss: 0.0615\n",
      "Epoch 7, Batch 1270, Loss: 0.0469\n",
      "Epoch 7, Batch 1280, Loss: 0.0742\n",
      "Epoch 7, Batch 1290, Loss: 0.0689\n",
      "Epoch 7, Batch 1300, Loss: 0.0476\n",
      "Epoch 7, Batch 1310, Loss: 0.0314\n",
      "Epoch 7, Batch 1320, Loss: 0.1067\n",
      "Epoch 7, Batch 1330, Loss: 0.0409\n",
      "Epoch 7, Batch 1340, Loss: 0.0650\n",
      "Epoch 7, Batch 1350, Loss: 0.0973\n",
      "Epoch 7, Batch 1360, Loss: 0.2143\n",
      "Epoch 7, Batch 1370, Loss: 0.0378\n",
      "Epoch 7, Batch 1380, Loss: 0.0374\n",
      "Epoch 7, Batch 1390, Loss: 0.0482\n",
      "Epoch 7, Batch 1400, Loss: 0.0429\n",
      "Epoch 7, Batch 1410, Loss: 0.1124\n",
      "Epoch 7, Batch 1420, Loss: 0.1078\n",
      "Epoch 7, Batch 1430, Loss: 0.0558\n",
      "Epoch 7, Batch 1440, Loss: 0.0931\n",
      "Epoch 7, Batch 1450, Loss: 0.0575\n",
      "Epoch 7, Batch 1460, Loss: 0.0449\n",
      "\n",
      "Epoch 7/50, Average Loss: 0.0939\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved for Epoch 7\n",
      "\n",
      "\n",
      "Epoch 8, Batch 0, Loss: 0.0694\n",
      "Epoch 8, Batch 10, Loss: 0.1230\n",
      "Epoch 8, Batch 20, Loss: 0.0503\n",
      "Epoch 8, Batch 30, Loss: 0.0347\n",
      "Epoch 8, Batch 40, Loss: 0.0954\n",
      "Epoch 8, Batch 50, Loss: 0.0438\n",
      "Epoch 8, Batch 60, Loss: 0.0779\n",
      "Epoch 8, Batch 70, Loss: 0.0424\n",
      "Epoch 8, Batch 80, Loss: 0.0643\n",
      "Epoch 8, Batch 90, Loss: 0.0368\n",
      "Epoch 8, Batch 100, Loss: 0.0303\n",
      "Epoch 8, Batch 110, Loss: 0.0532\n",
      "Epoch 8, Batch 120, Loss: 0.0471\n",
      "Epoch 8, Batch 130, Loss: 0.0587\n",
      "Epoch 8, Batch 140, Loss: 0.7025\n",
      "Epoch 8, Batch 150, Loss: 0.0681\n",
      "Epoch 8, Batch 160, Loss: 0.0932\n",
      "Epoch 8, Batch 170, Loss: 0.0800\n",
      "Epoch 8, Batch 180, Loss: 0.0478\n",
      "Epoch 8, Batch 190, Loss: 0.0474\n",
      "Epoch 8, Batch 200, Loss: 0.0449\n",
      "Epoch 8, Batch 210, Loss: 0.0606\n",
      "Epoch 8, Batch 220, Loss: 0.1102\n",
      "Epoch 8, Batch 230, Loss: 0.0962\n",
      "Epoch 8, Batch 240, Loss: 0.1135\n",
      "Epoch 8, Batch 250, Loss: 0.0507\n",
      "Epoch 8, Batch 260, Loss: 0.0353\n",
      "Epoch 8, Batch 270, Loss: 0.5135\n",
      "Epoch 8, Batch 280, Loss: 0.0490\n",
      "Epoch 8, Batch 290, Loss: 0.0725\n",
      "Epoch 8, Batch 300, Loss: 0.0969\n",
      "Epoch 8, Batch 310, Loss: 0.0422\n",
      "Epoch 8, Batch 320, Loss: 0.0637\n",
      "Epoch 8, Batch 330, Loss: 0.0488\n",
      "Epoch 8, Batch 340, Loss: 0.6664\n",
      "Epoch 8, Batch 350, Loss: 0.0536\n",
      "Epoch 8, Batch 360, Loss: 0.0716\n",
      "Epoch 8, Batch 370, Loss: 0.0594\n",
      "Epoch 8, Batch 380, Loss: 0.5358\n",
      "Epoch 8, Batch 390, Loss: 0.0448\n",
      "Epoch 8, Batch 400, Loss: 0.0488\n",
      "Epoch 8, Batch 410, Loss: 0.1925\n",
      "Epoch 8, Batch 420, Loss: 0.0524\n",
      "Epoch 8, Batch 430, Loss: 0.1347\n",
      "Epoch 8, Batch 440, Loss: 0.0815\n",
      "Epoch 8, Batch 450, Loss: 0.0824\n",
      "Epoch 8, Batch 460, Loss: 0.0534\n",
      "Epoch 8, Batch 470, Loss: 0.0230\n",
      "Epoch 8, Batch 480, Loss: 0.3837\n",
      "Epoch 8, Batch 490, Loss: 0.0895\n",
      "Epoch 8, Batch 500, Loss: 0.0627\n",
      "Epoch 8, Batch 510, Loss: 0.0614\n",
      "Epoch 8, Batch 520, Loss: 0.1521\n",
      "Epoch 8, Batch 530, Loss: 0.0886\n",
      "Epoch 8, Batch 540, Loss: 0.0500\n",
      "Epoch 8, Batch 550, Loss: 0.0873\n",
      "Epoch 8, Batch 560, Loss: 0.0486\n",
      "Epoch 8, Batch 570, Loss: 0.1469\n",
      "Epoch 8, Batch 580, Loss: 0.0468\n",
      "Epoch 8, Batch 590, Loss: 0.0601\n",
      "Epoch 8, Batch 600, Loss: 0.0464\n",
      "Epoch 8, Batch 610, Loss: 0.0503\n",
      "Epoch 8, Batch 620, Loss: 0.0653\n",
      "Epoch 8, Batch 630, Loss: 0.0526\n",
      "Epoch 8, Batch 640, Loss: 0.0608\n",
      "Epoch 8, Batch 650, Loss: 0.1084\n",
      "Epoch 8, Batch 660, Loss: 0.0865\n",
      "Epoch 8, Batch 670, Loss: 0.1179\n",
      "Epoch 8, Batch 680, Loss: 0.0766\n",
      "Epoch 8, Batch 690, Loss: 0.0581\n",
      "Epoch 8, Batch 700, Loss: 0.2875\n",
      "Epoch 8, Batch 710, Loss: 0.0646\n",
      "Epoch 8, Batch 720, Loss: 0.1106\n",
      "Epoch 8, Batch 730, Loss: 0.0532\n",
      "Epoch 8, Batch 740, Loss: 0.1344\n",
      "Epoch 8, Batch 750, Loss: 0.0968\n",
      "Epoch 8, Batch 760, Loss: 1.2540\n",
      "Epoch 8, Batch 770, Loss: 0.0408\n",
      "Epoch 8, Batch 780, Loss: 0.0757\n",
      "Epoch 8, Batch 790, Loss: 0.0715\n",
      "Epoch 8, Batch 800, Loss: 0.1252\n",
      "Epoch 8, Batch 810, Loss: 0.0533\n",
      "Epoch 8, Batch 820, Loss: 0.0486\n",
      "Epoch 8, Batch 830, Loss: 0.0508\n",
      "Epoch 8, Batch 840, Loss: 0.0483\n",
      "Epoch 8, Batch 850, Loss: 0.1309\n",
      "Epoch 8, Batch 860, Loss: 0.0468\n",
      "Epoch 8, Batch 870, Loss: 0.0758\n",
      "Epoch 8, Batch 880, Loss: 0.0530\n",
      "Epoch 8, Batch 890, Loss: 0.0932\n",
      "Epoch 8, Batch 900, Loss: 0.0336\n",
      "Epoch 8, Batch 910, Loss: 0.1164\n",
      "Epoch 8, Batch 920, Loss: 0.0384\n",
      "Epoch 8, Batch 930, Loss: 0.0482\n",
      "Epoch 8, Batch 940, Loss: 0.0512\n",
      "Epoch 8, Batch 950, Loss: 0.0882\n",
      "Epoch 8, Batch 960, Loss: 0.0809\n",
      "Epoch 8, Batch 970, Loss: 0.0733\n",
      "Epoch 8, Batch 980, Loss: 0.0275\n",
      "Epoch 8, Batch 990, Loss: 0.1182\n",
      "Epoch 8, Batch 1000, Loss: 0.0779\n",
      "Epoch 8, Batch 1010, Loss: 0.2481\n",
      "Epoch 8, Batch 1020, Loss: 0.1409\n",
      "Epoch 8, Batch 1030, Loss: 0.0532\n",
      "Epoch 8, Batch 1040, Loss: 0.0471\n",
      "Epoch 8, Batch 1050, Loss: 0.0561\n",
      "Epoch 8, Batch 1060, Loss: 0.0921\n",
      "Epoch 8, Batch 1070, Loss: 0.0539\n",
      "Epoch 8, Batch 1080, Loss: 0.1622\n",
      "Epoch 8, Batch 1090, Loss: 0.0325\n",
      "Epoch 8, Batch 1100, Loss: 0.0734\n",
      "Epoch 8, Batch 1110, Loss: 1.1391\n",
      "Epoch 8, Batch 1120, Loss: 0.0783\n",
      "Epoch 8, Batch 1130, Loss: 0.0305\n",
      "Epoch 8, Batch 1140, Loss: 0.0538\n",
      "Epoch 8, Batch 1150, Loss: 0.0427\n",
      "Epoch 8, Batch 1160, Loss: 0.5521\n",
      "Epoch 8, Batch 1170, Loss: 0.0539\n",
      "Epoch 8, Batch 1180, Loss: 0.1139\n",
      "Epoch 8, Batch 1190, Loss: 0.0771\n",
      "Epoch 8, Batch 1200, Loss: 0.0564\n",
      "Epoch 8, Batch 1210, Loss: 0.1828\n",
      "Epoch 8, Batch 1220, Loss: 0.0867\n",
      "Epoch 8, Batch 1230, Loss: 0.0792\n",
      "Epoch 8, Batch 1240, Loss: 0.0685\n",
      "Epoch 8, Batch 1250, Loss: 0.0261\n",
      "Epoch 8, Batch 1260, Loss: 0.0342\n",
      "Epoch 8, Batch 1270, Loss: 0.0412\n",
      "Epoch 8, Batch 1280, Loss: 0.0894\n",
      "Epoch 8, Batch 1290, Loss: 0.0862\n",
      "Epoch 8, Batch 1300, Loss: 0.1564\n",
      "Epoch 8, Batch 1310, Loss: 0.0546\n",
      "Epoch 8, Batch 1320, Loss: 0.1114\n",
      "Epoch 8, Batch 1330, Loss: 0.4412\n",
      "Epoch 8, Batch 1340, Loss: 0.1083\n",
      "Epoch 8, Batch 1350, Loss: 0.0553\n",
      "Epoch 8, Batch 1360, Loss: 0.1160\n",
      "Epoch 8, Batch 1370, Loss: 0.0324\n",
      "Epoch 8, Batch 1380, Loss: 0.0345\n",
      "Epoch 8, Batch 1390, Loss: 0.0570\n",
      "Epoch 8, Batch 1400, Loss: 0.0599\n",
      "Epoch 8, Batch 1410, Loss: 0.0683\n",
      "Epoch 8, Batch 1420, Loss: 0.0996\n",
      "Epoch 8, Batch 1430, Loss: 0.0755\n",
      "Epoch 8, Batch 1440, Loss: 0.0687\n",
      "Epoch 8, Batch 1450, Loss: 0.0726\n",
      "Epoch 8, Batch 1460, Loss: 0.0400\n",
      "\n",
      "Epoch 8/50, Average Loss: 0.0920\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved for Epoch 8\n",
      "\n",
      "\n",
      "Epoch 9, Batch 0, Loss: 0.0988\n",
      "Epoch 9, Batch 10, Loss: 0.0448\n",
      "Epoch 9, Batch 20, Loss: 0.1059\n",
      "Epoch 9, Batch 30, Loss: 0.0588\n",
      "Epoch 9, Batch 40, Loss: 0.1006\n",
      "Epoch 9, Batch 50, Loss: 0.0418\n",
      "Epoch 9, Batch 60, Loss: 0.0376\n",
      "Epoch 9, Batch 70, Loss: 0.1105\n",
      "Epoch 9, Batch 80, Loss: 0.0509\n",
      "Epoch 9, Batch 90, Loss: 0.1126\n",
      "Epoch 9, Batch 100, Loss: 0.0623\n",
      "Epoch 9, Batch 110, Loss: 0.0572\n",
      "Epoch 9, Batch 120, Loss: 0.0436\n",
      "Epoch 9, Batch 130, Loss: 0.0412\n",
      "Epoch 9, Batch 140, Loss: 0.0315\n",
      "Epoch 9, Batch 150, Loss: 0.0290\n",
      "Epoch 9, Batch 160, Loss: 0.0404\n",
      "Epoch 9, Batch 170, Loss: 0.1154\n",
      "Epoch 9, Batch 180, Loss: 0.0758\n",
      "Epoch 9, Batch 190, Loss: 0.1083\n",
      "Epoch 9, Batch 200, Loss: 0.0675\n",
      "Epoch 9, Batch 210, Loss: 0.0586\n",
      "Epoch 9, Batch 220, Loss: 0.0414\n",
      "Epoch 9, Batch 230, Loss: 0.0822\n",
      "Epoch 9, Batch 240, Loss: 0.1445\n",
      "Epoch 9, Batch 250, Loss: 0.0631\n",
      "Epoch 9, Batch 260, Loss: 0.0664\n",
      "Epoch 9, Batch 270, Loss: 0.0534\n",
      "Epoch 9, Batch 280, Loss: 0.0686\n",
      "Epoch 9, Batch 290, Loss: 0.2049\n",
      "Epoch 9, Batch 300, Loss: 0.0388\n",
      "Epoch 9, Batch 310, Loss: 0.0411\n",
      "Epoch 9, Batch 320, Loss: 0.0580\n",
      "Epoch 9, Batch 330, Loss: 0.1094\n",
      "Epoch 9, Batch 340, Loss: 0.0912\n",
      "Epoch 9, Batch 350, Loss: 0.1160\n",
      "Epoch 9, Batch 360, Loss: 0.0462\n",
      "Epoch 9, Batch 370, Loss: 0.0787\n",
      "Epoch 9, Batch 380, Loss: 0.0650\n",
      "Epoch 9, Batch 390, Loss: 0.0948\n",
      "Epoch 9, Batch 400, Loss: 0.1159\n",
      "Epoch 9, Batch 410, Loss: 0.0759\n",
      "Epoch 9, Batch 420, Loss: 0.1105\n",
      "Epoch 9, Batch 430, Loss: 0.0583\n",
      "Epoch 9, Batch 440, Loss: 0.0538\n",
      "Epoch 9, Batch 450, Loss: 0.0908\n",
      "Epoch 9, Batch 460, Loss: 0.0432\n",
      "Epoch 9, Batch 470, Loss: 0.0793\n",
      "Epoch 9, Batch 480, Loss: 0.0547\n",
      "Epoch 9, Batch 490, Loss: 0.0775\n",
      "Epoch 9, Batch 500, Loss: 0.0368\n",
      "Epoch 9, Batch 510, Loss: 0.1464\n",
      "Epoch 9, Batch 520, Loss: 0.0460\n",
      "Epoch 9, Batch 530, Loss: 0.0518\n",
      "Epoch 9, Batch 540, Loss: 0.0557\n",
      "Epoch 9, Batch 550, Loss: 0.0961\n",
      "Epoch 9, Batch 560, Loss: 0.0990\n",
      "Epoch 9, Batch 570, Loss: 0.0560\n",
      "Epoch 9, Batch 580, Loss: 0.0395\n",
      "Epoch 9, Batch 590, Loss: 0.0498\n",
      "Epoch 9, Batch 600, Loss: 0.0543\n",
      "Epoch 9, Batch 610, Loss: 0.0751\n",
      "Epoch 9, Batch 620, Loss: 0.0766\n",
      "Epoch 9, Batch 630, Loss: 0.1030\n",
      "Epoch 9, Batch 640, Loss: 0.0429\n",
      "Epoch 9, Batch 650, Loss: 0.0468\n",
      "Epoch 9, Batch 660, Loss: 0.1041\n",
      "Epoch 9, Batch 670, Loss: 0.0391\n",
      "Epoch 9, Batch 680, Loss: 0.0499\n",
      "Epoch 9, Batch 690, Loss: 0.0518\n",
      "Epoch 9, Batch 700, Loss: 0.0346\n",
      "Epoch 9, Batch 710, Loss: 0.0489\n",
      "Epoch 9, Batch 720, Loss: 0.1615\n",
      "Epoch 9, Batch 730, Loss: 0.0274\n",
      "Epoch 9, Batch 740, Loss: 0.0714\n",
      "Epoch 9, Batch 750, Loss: 0.0706\n",
      "Epoch 9, Batch 760, Loss: 0.0709\n",
      "Epoch 9, Batch 770, Loss: 0.0446\n",
      "Epoch 9, Batch 780, Loss: 0.0661\n",
      "Epoch 9, Batch 790, Loss: 0.0342\n",
      "Epoch 9, Batch 800, Loss: 0.0499\n",
      "Epoch 9, Batch 810, Loss: 0.0832\n",
      "Epoch 9, Batch 820, Loss: 0.1082\n",
      "Epoch 9, Batch 830, Loss: 0.0643\n",
      "Epoch 9, Batch 840, Loss: 0.0405\n",
      "Epoch 9, Batch 850, Loss: 0.0670\n",
      "Epoch 9, Batch 860, Loss: 0.1111\n",
      "Epoch 9, Batch 870, Loss: 0.0489\n",
      "Epoch 9, Batch 880, Loss: 0.0604\n",
      "Epoch 9, Batch 890, Loss: 0.0642\n",
      "Epoch 9, Batch 900, Loss: 0.0799\n",
      "Epoch 9, Batch 910, Loss: 0.2462\n",
      "Epoch 9, Batch 920, Loss: 0.0548\n",
      "Epoch 9, Batch 930, Loss: 0.1031\n",
      "Epoch 9, Batch 940, Loss: 0.0532\n",
      "Epoch 9, Batch 950, Loss: 0.0459\n",
      "Epoch 9, Batch 960, Loss: 0.0452\n",
      "Epoch 9, Batch 970, Loss: 0.0513\n",
      "Epoch 9, Batch 980, Loss: 0.1719\n",
      "Epoch 9, Batch 990, Loss: 0.0353\n",
      "Epoch 9, Batch 1000, Loss: 0.0668\n",
      "Epoch 9, Batch 1010, Loss: 0.0622\n",
      "Epoch 9, Batch 1020, Loss: 0.3977\n",
      "Epoch 9, Batch 1030, Loss: 0.1429\n",
      "Epoch 9, Batch 1040, Loss: 0.0593\n",
      "Epoch 9, Batch 1050, Loss: 0.1082\n",
      "Epoch 9, Batch 1060, Loss: 0.0451\n",
      "Epoch 9, Batch 1070, Loss: 0.0439\n",
      "Epoch 9, Batch 1080, Loss: 0.0402\n",
      "Epoch 9, Batch 1090, Loss: 0.0512\n",
      "Epoch 9, Batch 1100, Loss: 0.0442\n",
      "Epoch 9, Batch 1110, Loss: 0.1035\n",
      "Epoch 9, Batch 1120, Loss: 0.0486\n",
      "Epoch 9, Batch 1130, Loss: 0.0290\n",
      "Epoch 9, Batch 1140, Loss: 0.0530\n",
      "Epoch 9, Batch 1150, Loss: 0.0474\n",
      "Epoch 9, Batch 1160, Loss: 0.0533\n",
      "Epoch 9, Batch 1170, Loss: 0.0647\n",
      "Epoch 9, Batch 1180, Loss: 0.0897\n",
      "Epoch 9, Batch 1190, Loss: 0.0632\n",
      "Epoch 9, Batch 1200, Loss: 0.3625\n",
      "Epoch 9, Batch 1210, Loss: 0.0638\n",
      "Epoch 9, Batch 1220, Loss: 0.0696\n",
      "Epoch 9, Batch 1230, Loss: 0.0822\n",
      "Epoch 9, Batch 1240, Loss: 0.0546\n",
      "Epoch 9, Batch 1250, Loss: 0.1691\n",
      "Epoch 9, Batch 1260, Loss: 0.1487\n",
      "Epoch 9, Batch 1270, Loss: 0.0485\n",
      "Epoch 9, Batch 1280, Loss: 0.0577\n",
      "Epoch 9, Batch 1290, Loss: 0.1158\n",
      "Epoch 9, Batch 1300, Loss: 0.0607\n",
      "Epoch 9, Batch 1310, Loss: 0.0605\n",
      "Epoch 9, Batch 1320, Loss: 0.0398\n",
      "Epoch 9, Batch 1330, Loss: 0.0429\n",
      "Epoch 9, Batch 1340, Loss: 0.0489\n",
      "Epoch 9, Batch 1350, Loss: 0.0871\n",
      "Epoch 9, Batch 1360, Loss: 0.0404\n",
      "Epoch 9, Batch 1370, Loss: 0.0547\n",
      "Epoch 9, Batch 1380, Loss: 0.0536\n",
      "Epoch 9, Batch 1390, Loss: 0.1876\n",
      "Epoch 9, Batch 1400, Loss: 0.0539\n",
      "Epoch 9, Batch 1410, Loss: 0.0643\n",
      "Epoch 9, Batch 1420, Loss: 0.0592\n",
      "Epoch 9, Batch 1430, Loss: 0.0500\n",
      "Epoch 9, Batch 1440, Loss: 0.0833\n",
      "Epoch 9, Batch 1450, Loss: 0.0907\n",
      "Epoch 9, Batch 1460, Loss: 0.1116\n",
      "\n",
      "Epoch 9/50, Average Loss: 0.0908\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved for Epoch 9\n",
      "\n",
      "\n",
      "Epoch 10, Batch 0, Loss: 0.0365\n",
      "Epoch 10, Batch 10, Loss: 0.0748\n",
      "Epoch 10, Batch 20, Loss: 0.0428\n",
      "Epoch 10, Batch 30, Loss: 0.0202\n",
      "Epoch 10, Batch 40, Loss: 0.1476\n",
      "Epoch 10, Batch 50, Loss: 0.0836\n",
      "Epoch 10, Batch 60, Loss: 0.0285\n",
      "Epoch 10, Batch 70, Loss: 0.0719\n",
      "Epoch 10, Batch 80, Loss: 0.0780\n",
      "Epoch 10, Batch 90, Loss: 0.0407\n",
      "Epoch 10, Batch 100, Loss: 0.0636\n",
      "Epoch 10, Batch 110, Loss: 0.0499\n",
      "Epoch 10, Batch 120, Loss: 0.0400\n",
      "Epoch 10, Batch 130, Loss: 0.0651\n",
      "Epoch 10, Batch 140, Loss: 0.0686\n",
      "Epoch 10, Batch 150, Loss: 0.0467\n",
      "Epoch 10, Batch 160, Loss: 0.0572\n",
      "Epoch 10, Batch 170, Loss: 0.0722\n",
      "Epoch 10, Batch 180, Loss: 0.0619\n",
      "Epoch 10, Batch 190, Loss: 0.0575\n",
      "Epoch 10, Batch 200, Loss: 0.0583\n",
      "Epoch 10, Batch 210, Loss: 0.2041\n",
      "Epoch 10, Batch 220, Loss: 0.0744\n",
      "Epoch 10, Batch 230, Loss: 0.0417\n",
      "Epoch 10, Batch 240, Loss: 0.1008\n",
      "Epoch 10, Batch 250, Loss: 0.0662\n",
      "Epoch 10, Batch 260, Loss: 0.0332\n",
      "Epoch 10, Batch 270, Loss: 0.0390\n",
      "Epoch 10, Batch 280, Loss: 0.1217\n",
      "Epoch 10, Batch 290, Loss: 0.0651\n",
      "Epoch 10, Batch 300, Loss: 0.0510\n",
      "Epoch 10, Batch 310, Loss: 0.0738\n",
      "Epoch 10, Batch 320, Loss: 0.1535\n",
      "Epoch 10, Batch 330, Loss: 0.0662\n",
      "Epoch 10, Batch 340, Loss: 0.0665\n",
      "Epoch 10, Batch 350, Loss: 0.0803\n",
      "Epoch 10, Batch 360, Loss: 0.0579\n",
      "Epoch 10, Batch 370, Loss: 0.0899\n",
      "Epoch 10, Batch 380, Loss: 0.0305\n",
      "Epoch 10, Batch 390, Loss: 0.0237\n",
      "Epoch 10, Batch 400, Loss: 0.0863\n",
      "Epoch 10, Batch 410, Loss: 0.1228\n",
      "Epoch 10, Batch 420, Loss: 0.0574\n",
      "Epoch 10, Batch 430, Loss: 0.0731\n",
      "Epoch 10, Batch 440, Loss: 0.0422\n",
      "Epoch 10, Batch 450, Loss: 0.0320\n",
      "Epoch 10, Batch 460, Loss: 0.0504\n",
      "Epoch 10, Batch 470, Loss: 0.0427\n",
      "Epoch 10, Batch 480, Loss: 0.3570\n",
      "Epoch 10, Batch 490, Loss: 0.0564\n",
      "Epoch 10, Batch 500, Loss: 0.0648\n",
      "Epoch 10, Batch 510, Loss: 0.0289\n",
      "Epoch 10, Batch 520, Loss: 0.0816\n",
      "Epoch 10, Batch 530, Loss: 0.0649\n",
      "Epoch 10, Batch 540, Loss: 0.0729\n",
      "Epoch 10, Batch 550, Loss: 0.0313\n",
      "Epoch 10, Batch 560, Loss: 0.1374\n",
      "Epoch 10, Batch 570, Loss: 0.0446\n",
      "Epoch 10, Batch 580, Loss: 0.0619\n",
      "Epoch 10, Batch 590, Loss: 0.0563\n",
      "Epoch 10, Batch 600, Loss: 0.0298\n",
      "Epoch 10, Batch 610, Loss: 0.0761\n",
      "Epoch 10, Batch 620, Loss: 0.1516\n",
      "Epoch 10, Batch 630, Loss: 0.0889\n",
      "Epoch 10, Batch 640, Loss: 0.0718\n",
      "Epoch 10, Batch 650, Loss: 0.0766\n",
      "Epoch 10, Batch 660, Loss: 0.0880\n",
      "Epoch 10, Batch 670, Loss: 0.4770\n",
      "Epoch 10, Batch 680, Loss: 0.1716\n",
      "Epoch 10, Batch 690, Loss: 0.0521\n",
      "Epoch 10, Batch 700, Loss: 0.0669\n",
      "Epoch 10, Batch 710, Loss: 0.0468\n",
      "Epoch 10, Batch 720, Loss: 0.5722\n",
      "Epoch 10, Batch 730, Loss: 0.0534\n",
      "Epoch 10, Batch 740, Loss: 0.0655\n",
      "Epoch 10, Batch 750, Loss: 0.0490\n",
      "Epoch 10, Batch 760, Loss: 0.0470\n",
      "Epoch 10, Batch 770, Loss: 0.0273\n",
      "Epoch 10, Batch 780, Loss: 0.0374\n",
      "Epoch 10, Batch 790, Loss: 0.0594\n",
      "Epoch 10, Batch 800, Loss: 0.0402\n",
      "Epoch 10, Batch 810, Loss: 0.1271\n",
      "Epoch 10, Batch 820, Loss: 0.0678\n",
      "Epoch 10, Batch 830, Loss: 0.0520\n",
      "Epoch 10, Batch 840, Loss: 0.0413\n",
      "Epoch 10, Batch 850, Loss: 0.0677\n",
      "Epoch 10, Batch 860, Loss: 0.0457\n",
      "Epoch 10, Batch 870, Loss: 0.0468\n",
      "Epoch 10, Batch 880, Loss: 0.0748\n",
      "Epoch 10, Batch 890, Loss: 0.0465\n",
      "Epoch 10, Batch 900, Loss: 0.0654\n",
      "Epoch 10, Batch 910, Loss: 0.3298\n",
      "Epoch 10, Batch 920, Loss: 0.0861\n",
      "Epoch 10, Batch 930, Loss: 0.2880\n",
      "Epoch 10, Batch 940, Loss: 0.0799\n",
      "Epoch 10, Batch 950, Loss: 0.0424\n",
      "Epoch 10, Batch 960, Loss: 0.0570\n",
      "Epoch 10, Batch 970, Loss: 0.1170\n",
      "Epoch 10, Batch 980, Loss: 0.0445\n",
      "Epoch 10, Batch 990, Loss: 0.0841\n",
      "Epoch 10, Batch 1000, Loss: 0.0776\n",
      "Epoch 10, Batch 1010, Loss: 0.0400\n",
      "Epoch 10, Batch 1020, Loss: 0.0422\n",
      "Epoch 10, Batch 1030, Loss: 0.0870\n",
      "Epoch 10, Batch 1040, Loss: 0.1226\n",
      "Epoch 10, Batch 1050, Loss: 0.0285\n",
      "Epoch 10, Batch 1060, Loss: 0.2216\n",
      "Epoch 10, Batch 1070, Loss: 0.0696\n",
      "Epoch 10, Batch 1080, Loss: 0.0919\n",
      "Epoch 10, Batch 1090, Loss: 0.0739\n",
      "Epoch 10, Batch 1100, Loss: 0.0987\n",
      "Epoch 10, Batch 1110, Loss: 0.1069\n",
      "Epoch 10, Batch 1120, Loss: 0.0701\n",
      "Epoch 10, Batch 1130, Loss: 0.0441\n",
      "Epoch 10, Batch 1140, Loss: 0.0926\n",
      "Epoch 10, Batch 1150, Loss: 0.0462\n",
      "Epoch 10, Batch 1160, Loss: 0.0786\n",
      "Epoch 10, Batch 1170, Loss: 0.1042\n",
      "Epoch 10, Batch 1180, Loss: 0.0535\n",
      "Epoch 10, Batch 1190, Loss: 0.0488\n",
      "Epoch 10, Batch 1200, Loss: 0.0860\n",
      "Epoch 10, Batch 1210, Loss: 0.0787\n",
      "Epoch 10, Batch 1220, Loss: 0.0567\n",
      "Epoch 10, Batch 1230, Loss: 0.0564\n",
      "Epoch 10, Batch 1240, Loss: 0.0701\n",
      "Epoch 10, Batch 1250, Loss: 0.0469\n",
      "Epoch 10, Batch 1260, Loss: 0.1660\n",
      "Epoch 10, Batch 1270, Loss: 0.0476\n",
      "Epoch 10, Batch 1280, Loss: 0.0521\n",
      "Epoch 10, Batch 1290, Loss: 0.0800\n",
      "Epoch 10, Batch 1300, Loss: 0.1025\n",
      "Epoch 10, Batch 1310, Loss: 0.0405\n",
      "Epoch 10, Batch 1320, Loss: 0.1209\n",
      "Epoch 10, Batch 1330, Loss: 0.0919\n",
      "Epoch 10, Batch 1340, Loss: 0.0778\n",
      "Epoch 10, Batch 1350, Loss: 0.0512\n",
      "Epoch 10, Batch 1360, Loss: 0.0960\n",
      "Epoch 10, Batch 1370, Loss: 0.0838\n",
      "Epoch 10, Batch 1380, Loss: 0.0652\n",
      "Epoch 10, Batch 1390, Loss: 0.0363\n",
      "Epoch 10, Batch 1400, Loss: 0.3842\n",
      "Epoch 10, Batch 1410, Loss: 0.0587\n",
      "Epoch 10, Batch 1420, Loss: 0.0580\n",
      "Epoch 10, Batch 1430, Loss: 0.0362\n",
      "Epoch 10, Batch 1440, Loss: 0.1120\n",
      "Epoch 10, Batch 1450, Loss: 0.0843\n",
      "Epoch 10, Batch 1460, Loss: 0.0764\n",
      "\n",
      "Epoch 10/50, Average Loss: 0.0903\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved for Epoch 10\n",
      "\n",
      "\n",
      "Epoch 11, Batch 0, Loss: 0.0951\n",
      "Epoch 11, Batch 10, Loss: 0.0774\n",
      "Epoch 11, Batch 20, Loss: 0.0773\n",
      "Epoch 11, Batch 30, Loss: 0.0597\n",
      "Epoch 11, Batch 40, Loss: 1.2308\n",
      "Epoch 11, Batch 50, Loss: 0.0318\n",
      "Epoch 11, Batch 60, Loss: 0.0273\n",
      "Epoch 11, Batch 70, Loss: 0.0334\n",
      "Epoch 11, Batch 80, Loss: 0.0524\n",
      "Epoch 11, Batch 90, Loss: 0.1397\n",
      "Epoch 11, Batch 100, Loss: 0.1357\n",
      "Epoch 11, Batch 110, Loss: 0.0457\n",
      "Epoch 11, Batch 120, Loss: 0.2877\n",
      "Epoch 11, Batch 130, Loss: 0.0439\n",
      "Epoch 11, Batch 140, Loss: 0.0797\n",
      "Epoch 11, Batch 150, Loss: 0.2267\n",
      "Epoch 11, Batch 160, Loss: 0.0631\n",
      "Epoch 11, Batch 170, Loss: 0.0801\n",
      "Epoch 11, Batch 180, Loss: 0.0394\n",
      "Epoch 11, Batch 190, Loss: 0.0988\n",
      "Epoch 11, Batch 200, Loss: 0.0447\n",
      "Epoch 11, Batch 210, Loss: 0.1217\n",
      "Epoch 11, Batch 220, Loss: 0.0540\n",
      "Epoch 11, Batch 230, Loss: 0.0343\n",
      "Epoch 11, Batch 240, Loss: 0.0814\n",
      "Epoch 11, Batch 250, Loss: 0.0560\n",
      "Epoch 11, Batch 260, Loss: 0.0555\n",
      "Epoch 11, Batch 270, Loss: 0.1121\n",
      "Epoch 11, Batch 280, Loss: 0.0597\n",
      "Epoch 11, Batch 290, Loss: 0.0639\n",
      "Epoch 11, Batch 300, Loss: 0.2117\n",
      "Epoch 11, Batch 310, Loss: 0.0362\n",
      "Epoch 11, Batch 320, Loss: 0.0315\n",
      "Epoch 11, Batch 330, Loss: 0.0483\n",
      "Epoch 11, Batch 340, Loss: 0.1325\n",
      "Epoch 11, Batch 350, Loss: 0.0572\n",
      "Epoch 11, Batch 360, Loss: 0.0311\n",
      "Epoch 11, Batch 370, Loss: 0.0515\n",
      "Epoch 11, Batch 380, Loss: 0.0545\n",
      "Epoch 11, Batch 390, Loss: 0.0563\n",
      "Epoch 11, Batch 400, Loss: 0.0402\n",
      "Epoch 11, Batch 410, Loss: 0.0363\n",
      "Epoch 11, Batch 420, Loss: 0.0777\n",
      "Epoch 11, Batch 430, Loss: 0.2177\n",
      "Epoch 11, Batch 440, Loss: 0.0923\n",
      "Epoch 11, Batch 450, Loss: 0.0901\n",
      "Epoch 11, Batch 460, Loss: 0.0439\n",
      "Epoch 11, Batch 470, Loss: 0.0432\n",
      "Epoch 11, Batch 480, Loss: 0.0645\n",
      "Epoch 11, Batch 490, Loss: 0.5283\n",
      "Epoch 11, Batch 500, Loss: 0.1354\n",
      "Epoch 11, Batch 510, Loss: 0.0843\n",
      "Epoch 11, Batch 520, Loss: 0.0402\n",
      "Epoch 11, Batch 530, Loss: 0.0465\n",
      "Epoch 11, Batch 540, Loss: 0.2160\n",
      "Epoch 11, Batch 550, Loss: 0.0509\n",
      "Epoch 11, Batch 560, Loss: 0.0491\n",
      "Epoch 11, Batch 570, Loss: 0.0587\n",
      "Epoch 11, Batch 580, Loss: 0.0869\n",
      "Epoch 11, Batch 590, Loss: 0.0650\n",
      "Epoch 11, Batch 600, Loss: 0.2340\n",
      "Epoch 11, Batch 610, Loss: 0.0467\n",
      "Epoch 11, Batch 620, Loss: 0.0486\n",
      "Epoch 11, Batch 630, Loss: 0.1255\n",
      "Epoch 11, Batch 640, Loss: 0.1424\n",
      "Epoch 11, Batch 650, Loss: 0.0507\n",
      "Epoch 11, Batch 660, Loss: 0.0569\n",
      "Epoch 11, Batch 670, Loss: 0.0649\n",
      "Epoch 11, Batch 680, Loss: 0.0357\n",
      "Epoch 11, Batch 690, Loss: 0.1046\n",
      "Epoch 11, Batch 700, Loss: 0.0708\n",
      "Epoch 11, Batch 710, Loss: 0.1009\n",
      "Epoch 11, Batch 720, Loss: 0.0410\n",
      "Epoch 11, Batch 730, Loss: 0.0584\n",
      "Epoch 11, Batch 740, Loss: 0.0812\n",
      "Epoch 11, Batch 750, Loss: 0.0512\n",
      "Epoch 11, Batch 760, Loss: 0.0462\n",
      "Epoch 11, Batch 770, Loss: 0.1227\n",
      "Epoch 11, Batch 780, Loss: 0.0673\n",
      "Epoch 11, Batch 790, Loss: 0.0667\n",
      "Epoch 11, Batch 800, Loss: 0.0891\n",
      "Epoch 11, Batch 810, Loss: 0.0411\n",
      "Epoch 11, Batch 820, Loss: 0.1062\n",
      "Epoch 11, Batch 830, Loss: 0.0958\n",
      "Epoch 11, Batch 840, Loss: 0.0610\n",
      "Epoch 11, Batch 850, Loss: 0.0880\n",
      "Epoch 11, Batch 860, Loss: 0.1232\n",
      "Epoch 11, Batch 870, Loss: 0.0407\n",
      "Epoch 11, Batch 880, Loss: 0.0929\n",
      "Epoch 11, Batch 890, Loss: 0.0955\n",
      "Epoch 11, Batch 900, Loss: 0.0837\n",
      "Epoch 11, Batch 910, Loss: 0.0674\n",
      "Epoch 11, Batch 920, Loss: 0.0353\n",
      "Epoch 11, Batch 930, Loss: 0.0844\n",
      "Epoch 11, Batch 940, Loss: 0.0745\n",
      "Epoch 11, Batch 950, Loss: 0.0533\n",
      "Epoch 11, Batch 960, Loss: 0.0452\n",
      "Epoch 11, Batch 970, Loss: 0.0655\n",
      "Epoch 11, Batch 980, Loss: 0.0494\n",
      "Epoch 11, Batch 990, Loss: 0.0567\n",
      "Epoch 11, Batch 1000, Loss: 0.0379\n",
      "Epoch 11, Batch 1010, Loss: 0.0592\n",
      "Epoch 11, Batch 1020, Loss: 0.0384\n",
      "Epoch 11, Batch 1030, Loss: 0.4218\n",
      "Epoch 11, Batch 1040, Loss: 0.0738\n",
      "Epoch 11, Batch 1050, Loss: 0.0705\n",
      "Epoch 11, Batch 1060, Loss: 0.0715\n",
      "Epoch 11, Batch 1070, Loss: 0.0397\n",
      "Epoch 11, Batch 1080, Loss: 0.4392\n",
      "Epoch 11, Batch 1090, Loss: 0.0433\n",
      "Epoch 11, Batch 1100, Loss: 0.0661\n",
      "Epoch 11, Batch 1110, Loss: 0.0985\n",
      "Epoch 11, Batch 1120, Loss: 0.0801\n",
      "Epoch 11, Batch 1130, Loss: 0.0490\n",
      "Epoch 11, Batch 1140, Loss: 0.0835\n",
      "Epoch 11, Batch 1150, Loss: 0.0783\n",
      "Epoch 11, Batch 1160, Loss: 0.0938\n",
      "Epoch 11, Batch 1170, Loss: 0.4162\n",
      "Epoch 11, Batch 1180, Loss: 1.2266\n",
      "Epoch 11, Batch 1190, Loss: 0.0505\n",
      "Epoch 11, Batch 1200, Loss: 0.2252\n",
      "Epoch 11, Batch 1210, Loss: 0.0413\n",
      "Epoch 11, Batch 1220, Loss: 0.1196\n",
      "Epoch 11, Batch 1230, Loss: 0.0560\n",
      "Epoch 11, Batch 1240, Loss: 0.0511\n",
      "Epoch 11, Batch 1250, Loss: 0.0493\n",
      "Epoch 11, Batch 1260, Loss: 0.0832\n",
      "Epoch 11, Batch 1270, Loss: 0.0257\n",
      "Epoch 11, Batch 1280, Loss: 0.0411\n",
      "Epoch 11, Batch 1290, Loss: 0.0517\n",
      "Epoch 11, Batch 1300, Loss: 0.0738\n",
      "Epoch 11, Batch 1310, Loss: 0.0866\n",
      "Epoch 11, Batch 1320, Loss: 0.1630\n",
      "Epoch 11, Batch 1330, Loss: 0.0807\n",
      "Epoch 11, Batch 1340, Loss: 0.0731\n",
      "Epoch 11, Batch 1350, Loss: 0.1077\n",
      "Epoch 11, Batch 1360, Loss: 0.0881\n",
      "Epoch 11, Batch 1370, Loss: 0.1577\n",
      "Epoch 11, Batch 1380, Loss: 0.0725\n",
      "Epoch 11, Batch 1390, Loss: 0.0831\n",
      "Epoch 11, Batch 1400, Loss: 0.0627\n",
      "Epoch 11, Batch 1410, Loss: 0.0681\n",
      "Epoch 11, Batch 1420, Loss: 0.0367\n",
      "Epoch 11, Batch 1430, Loss: 0.0569\n",
      "Epoch 11, Batch 1440, Loss: 0.0777\n",
      "Epoch 11, Batch 1450, Loss: 0.3266\n",
      "Epoch 11, Batch 1460, Loss: 0.0665\n",
      "\n",
      "Epoch 11/50, Average Loss: 0.0891\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved for Epoch 11\n",
      "\n",
      "\n",
      "Epoch 12, Batch 0, Loss: 0.2108\n",
      "Epoch 12, Batch 10, Loss: 0.0209\n",
      "Epoch 12, Batch 20, Loss: 0.0541\n",
      "Epoch 12, Batch 30, Loss: 0.0384\n",
      "Epoch 12, Batch 40, Loss: 0.0285\n",
      "Epoch 12, Batch 50, Loss: 0.0362\n",
      "Epoch 12, Batch 60, Loss: 0.0475\n",
      "Epoch 12, Batch 70, Loss: 0.0495\n",
      "Epoch 12, Batch 80, Loss: 0.0394\n",
      "Epoch 12, Batch 90, Loss: 0.1329\n",
      "Epoch 12, Batch 100, Loss: 0.0521\n",
      "Epoch 12, Batch 110, Loss: 0.3956\n",
      "Epoch 12, Batch 120, Loss: 0.0639\n",
      "Epoch 12, Batch 130, Loss: 0.0408\n",
      "Epoch 12, Batch 140, Loss: 0.0882\n",
      "Epoch 12, Batch 150, Loss: 0.0466\n",
      "Epoch 12, Batch 160, Loss: 0.0417\n",
      "Epoch 12, Batch 170, Loss: 0.0479\n",
      "Epoch 12, Batch 180, Loss: 0.0960\n",
      "Epoch 12, Batch 190, Loss: 0.0462\n",
      "Epoch 12, Batch 200, Loss: 0.0809\n",
      "Epoch 12, Batch 210, Loss: 0.0535\n",
      "Epoch 12, Batch 220, Loss: 0.0733\n",
      "Epoch 12, Batch 230, Loss: 0.0720\n",
      "Epoch 12, Batch 240, Loss: 0.0760\n",
      "Epoch 12, Batch 250, Loss: 0.0443\n",
      "Epoch 12, Batch 260, Loss: 0.0692\n",
      "Epoch 12, Batch 270, Loss: 0.0810\n",
      "Epoch 12, Batch 280, Loss: 0.0445\n",
      "Epoch 12, Batch 290, Loss: 0.0414\n",
      "Epoch 12, Batch 300, Loss: 0.0669\n",
      "Epoch 12, Batch 310, Loss: 0.0688\n",
      "Epoch 12, Batch 320, Loss: 0.1070\n",
      "Epoch 12, Batch 330, Loss: 0.0492\n",
      "Epoch 12, Batch 340, Loss: 0.0369\n",
      "Epoch 12, Batch 350, Loss: 0.0666\n",
      "Epoch 12, Batch 360, Loss: 0.0602\n",
      "Epoch 12, Batch 370, Loss: 0.0639\n",
      "Epoch 12, Batch 380, Loss: 0.0702\n",
      "Epoch 12, Batch 390, Loss: 0.0776\n",
      "Epoch 12, Batch 400, Loss: 0.0316\n",
      "Epoch 12, Batch 410, Loss: 0.0398\n",
      "Epoch 12, Batch 420, Loss: 0.0416\n",
      "Epoch 12, Batch 430, Loss: 0.0618\n",
      "Epoch 12, Batch 440, Loss: 0.6471\n",
      "Epoch 12, Batch 450, Loss: 0.0409\n",
      "Epoch 12, Batch 460, Loss: 0.0645\n",
      "Epoch 12, Batch 470, Loss: 0.0512\n",
      "Epoch 12, Batch 480, Loss: 0.0926\n",
      "Epoch 12, Batch 490, Loss: 0.0537\n",
      "Epoch 12, Batch 500, Loss: 0.0950\n",
      "Epoch 12, Batch 510, Loss: 0.0529\n",
      "Epoch 12, Batch 520, Loss: 0.0781\n",
      "Epoch 12, Batch 530, Loss: 0.1231\n",
      "Epoch 12, Batch 540, Loss: 0.0712\n",
      "Epoch 12, Batch 550, Loss: 0.5162\n",
      "Epoch 12, Batch 560, Loss: 0.1169\n",
      "Epoch 12, Batch 570, Loss: 0.0920\n",
      "Epoch 12, Batch 580, Loss: 0.0687\n",
      "Epoch 12, Batch 590, Loss: 0.0780\n",
      "Epoch 12, Batch 600, Loss: 0.2020\n",
      "Epoch 12, Batch 610, Loss: 0.0447\n",
      "Epoch 12, Batch 620, Loss: 0.0674\n",
      "Epoch 12, Batch 630, Loss: 0.1389\n",
      "Epoch 12, Batch 640, Loss: 0.0473\n",
      "Epoch 12, Batch 650, Loss: 0.0540\n",
      "Epoch 12, Batch 660, Loss: 0.0760\n",
      "Epoch 12, Batch 670, Loss: 0.0534\n",
      "Epoch 12, Batch 680, Loss: 0.0639\n",
      "Epoch 12, Batch 690, Loss: 0.0541\n",
      "Epoch 12, Batch 700, Loss: 0.0451\n",
      "Epoch 12, Batch 710, Loss: 0.1067\n",
      "Epoch 12, Batch 720, Loss: 0.1780\n",
      "Epoch 12, Batch 730, Loss: 0.0662\n",
      "Epoch 12, Batch 740, Loss: 0.4792\n",
      "Epoch 12, Batch 750, Loss: 0.0710\n",
      "Epoch 12, Batch 760, Loss: 0.3692\n",
      "Epoch 12, Batch 770, Loss: 0.1108\n",
      "Epoch 12, Batch 780, Loss: 0.0785\n",
      "Epoch 12, Batch 790, Loss: 0.0451\n",
      "Epoch 12, Batch 800, Loss: 0.0463\n",
      "Epoch 12, Batch 810, Loss: 0.0482\n",
      "Epoch 12, Batch 820, Loss: 0.0461\n",
      "Epoch 12, Batch 830, Loss: 0.0392\n",
      "Epoch 12, Batch 840, Loss: 0.0881\n",
      "Epoch 12, Batch 850, Loss: 0.1309\n",
      "Epoch 12, Batch 860, Loss: 0.7815\n",
      "Epoch 12, Batch 870, Loss: 0.0584\n",
      "Epoch 12, Batch 880, Loss: 0.0791\n",
      "Epoch 12, Batch 890, Loss: 0.0429\n",
      "Epoch 12, Batch 900, Loss: 0.0644\n",
      "Epoch 12, Batch 910, Loss: 0.0759\n",
      "Epoch 12, Batch 920, Loss: 0.0677\n",
      "Epoch 12, Batch 930, Loss: 0.0408\n",
      "Epoch 12, Batch 940, Loss: 0.0450\n",
      "Epoch 12, Batch 950, Loss: 0.0550\n",
      "Epoch 12, Batch 960, Loss: 0.0546\n",
      "Epoch 12, Batch 970, Loss: 0.0717\n",
      "Epoch 12, Batch 980, Loss: 0.2966\n",
      "Epoch 12, Batch 990, Loss: 0.0879\n",
      "Epoch 12, Batch 1000, Loss: 0.0697\n",
      "Epoch 12, Batch 1010, Loss: 0.0371\n",
      "Epoch 12, Batch 1020, Loss: 0.1196\n",
      "Epoch 12, Batch 1030, Loss: 0.0473\n",
      "Epoch 12, Batch 1040, Loss: 0.0501\n",
      "Epoch 12, Batch 1050, Loss: 0.0315\n",
      "Epoch 12, Batch 1060, Loss: 0.0592\n",
      "Epoch 12, Batch 1070, Loss: 0.0669\n",
      "Epoch 12, Batch 1080, Loss: 0.0994\n",
      "Epoch 12, Batch 1090, Loss: 0.0707\n",
      "Epoch 12, Batch 1100, Loss: 0.0625\n",
      "Epoch 12, Batch 1110, Loss: 0.0530\n",
      "Epoch 12, Batch 1120, Loss: 0.0554\n",
      "Epoch 12, Batch 1130, Loss: 0.0799\n",
      "Epoch 12, Batch 1140, Loss: 0.0578\n",
      "Epoch 12, Batch 1150, Loss: 0.0457\n",
      "Epoch 12, Batch 1160, Loss: 0.0425\n",
      "Epoch 12, Batch 1170, Loss: 0.0956\n",
      "Epoch 12, Batch 1180, Loss: 0.1482\n",
      "Epoch 12, Batch 1190, Loss: 0.0824\n",
      "Epoch 12, Batch 1200, Loss: 0.0565\n",
      "Epoch 12, Batch 1210, Loss: 0.0817\n",
      "Epoch 12, Batch 1220, Loss: 0.0830\n",
      "Epoch 12, Batch 1230, Loss: 0.0704\n",
      "Epoch 12, Batch 1240, Loss: 0.0339\n",
      "Epoch 12, Batch 1250, Loss: 0.1815\n",
      "Epoch 12, Batch 1260, Loss: 0.0843\n",
      "Epoch 12, Batch 1270, Loss: 0.0393\n",
      "Epoch 12, Batch 1280, Loss: 0.0410\n",
      "Epoch 12, Batch 1290, Loss: 0.0317\n",
      "Epoch 12, Batch 1300, Loss: 0.0548\n",
      "Epoch 12, Batch 1310, Loss: 0.0445\n",
      "Epoch 12, Batch 1320, Loss: 0.1843\n",
      "Epoch 12, Batch 1330, Loss: 0.0492\n",
      "Epoch 12, Batch 1340, Loss: 0.0581\n",
      "Epoch 12, Batch 1350, Loss: 0.0575\n",
      "Epoch 12, Batch 1360, Loss: 0.1595\n",
      "Epoch 12, Batch 1370, Loss: 0.0624\n",
      "Epoch 12, Batch 1380, Loss: 0.0611\n",
      "Epoch 12, Batch 1390, Loss: 0.0445\n",
      "Epoch 12, Batch 1400, Loss: 0.1509\n",
      "Epoch 12, Batch 1410, Loss: 0.0784\n",
      "Epoch 12, Batch 1420, Loss: 0.0342\n",
      "Epoch 12, Batch 1430, Loss: 0.1075\n",
      "Epoch 12, Batch 1440, Loss: 0.0790\n",
      "Epoch 12, Batch 1450, Loss: 0.4898\n",
      "Epoch 12, Batch 1460, Loss: 0.0713\n",
      "\n",
      "Epoch 12/50, Average Loss: 0.0900\n",
      "\n",
      "\n",
      "\n",
      "Checkpoint saved for Epoch 12\n",
      "\n",
      "\n",
      "Epoch 13, Batch 0, Loss: 0.0726\n",
      "Epoch 13, Batch 10, Loss: 0.0499\n",
      "Epoch 13, Batch 20, Loss: 0.0404\n",
      "Epoch 13, Batch 30, Loss: 0.0624\n",
      "Epoch 13, Batch 40, Loss: 0.3225\n",
      "Epoch 13, Batch 50, Loss: 0.0539\n",
      "Epoch 13, Batch 60, Loss: 0.1908\n",
      "Epoch 13, Batch 70, Loss: 0.0576\n",
      "Epoch 13, Batch 80, Loss: 0.0426\n",
      "Epoch 13, Batch 90, Loss: 0.0579\n",
      "Epoch 13, Batch 100, Loss: 0.0481\n",
      "Epoch 13, Batch 110, Loss: 0.0521\n",
      "Epoch 13, Batch 120, Loss: 0.1312\n",
      "Epoch 13, Batch 130, Loss: 0.0482\n",
      "Epoch 13, Batch 140, Loss: 0.0611\n",
      "Epoch 13, Batch 150, Loss: 0.1077\n",
      "Epoch 13, Batch 160, Loss: 0.0350\n",
      "Epoch 13, Batch 170, Loss: 0.1023\n",
      "Epoch 13, Batch 180, Loss: 0.0507\n",
      "Epoch 13, Batch 190, Loss: 0.0523\n",
      "Epoch 13, Batch 200, Loss: 0.0339\n",
      "Epoch 13, Batch 210, Loss: 0.0240\n",
      "Epoch 13, Batch 220, Loss: 0.0802\n",
      "Epoch 13, Batch 230, Loss: 0.0457\n",
      "Epoch 13, Batch 240, Loss: 0.1191\n",
      "Epoch 13, Batch 250, Loss: 0.0730\n",
      "Epoch 13, Batch 260, Loss: 0.0574\n",
      "Epoch 13, Batch 270, Loss: 0.0500\n",
      "Epoch 13, Batch 280, Loss: 0.0449\n",
      "Epoch 13, Batch 290, Loss: 0.1203\n",
      "Epoch 13, Batch 300, Loss: 0.0588\n",
      "Epoch 13, Batch 310, Loss: 0.0761\n",
      "Epoch 13, Batch 320, Loss: 0.0408\n",
      "Epoch 13, Batch 330, Loss: 0.0426\n",
      "Epoch 13, Batch 340, Loss: 0.0506\n",
      "Epoch 13, Batch 350, Loss: 0.0712\n",
      "Epoch 13, Batch 360, Loss: 0.0679\n",
      "Epoch 13, Batch 370, Loss: 0.0438\n",
      "Epoch 13, Batch 380, Loss: 0.0700\n",
      "Epoch 13, Batch 390, Loss: 0.0366\n",
      "Epoch 13, Batch 400, Loss: 0.0392\n",
      "Epoch 13, Batch 410, Loss: 0.0551\n",
      "Epoch 13, Batch 420, Loss: 0.0522\n",
      "Epoch 13, Batch 430, Loss: 0.0988\n",
      "Epoch 13, Batch 440, Loss: 0.0744\n",
      "Epoch 13, Batch 450, Loss: 0.0769\n",
      "Epoch 13, Batch 460, Loss: 0.0698\n",
      "Epoch 13, Batch 470, Loss: 0.0506\n",
      "Epoch 13, Batch 480, Loss: 0.0957\n",
      "Epoch 13, Batch 490, Loss: 0.0359\n",
      "Epoch 13, Batch 500, Loss: 0.0416\n",
      "Epoch 13, Batch 510, Loss: 0.0374\n",
      "Epoch 13, Batch 520, Loss: 0.0598\n",
      "Epoch 13, Batch 530, Loss: 0.0550\n",
      "Epoch 13, Batch 540, Loss: 0.0268\n",
      "Epoch 13, Batch 550, Loss: 0.0389\n",
      "Epoch 13, Batch 560, Loss: 0.1027\n",
      "Epoch 13, Batch 570, Loss: 0.0922\n",
      "Epoch 13, Batch 580, Loss: 0.5225\n",
      "Epoch 13, Batch 590, Loss: 0.0724\n",
      "Epoch 13, Batch 600, Loss: 0.1090\n",
      "Epoch 13, Batch 610, Loss: 0.0364\n",
      "Epoch 13, Batch 620, Loss: 0.0434\n",
      "Epoch 13, Batch 630, Loss: 0.0561\n",
      "Epoch 13, Batch 640, Loss: 0.0369\n",
      "Epoch 13, Batch 650, Loss: 0.1021\n",
      "Epoch 13, Batch 660, Loss: 0.0477\n",
      "Epoch 13, Batch 670, Loss: 0.0450\n",
      "Epoch 13, Batch 680, Loss: 0.0544\n",
      "Epoch 13, Batch 690, Loss: 0.0733\n",
      "Epoch 13, Batch 700, Loss: 0.0463\n"
     ]
    }
   ],
   "source": [
    "# Training the Model with Resume Capability\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Path to save/load the checkpoint (using the same drive path you defined later is safer for Colab)\n",
    "checkpoint_path = \"/home/somel/code/FYP_Project/Training_checkpoints/run1/last_checkpoint.pth\" \n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0005)\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50 \n",
    "start_epoch = 0\n",
    "\n",
    "# --- RESUME LOGIC ---\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Found checkpoint at {checkpoint_path}. Loading...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # 1. Load Model Weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # 2. Load Optimizer State (Crucial for Adam!)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # 3. Load Scaler State (Crucial for Mixed Precision!)\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    \n",
    "    # 4. Set Start Epoch\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming training from Epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "# Function to process COCO target from list of annotations to dict\n",
    "def process_target(target):\n",
    "    if isinstance(target, dict):\n",
    "        return target\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    masks = []\n",
    "    for ann in target:\n",
    "        bbox = ann['bbox']\n",
    "        x, y, w, h = bbox\n",
    "        if w > 0 and h > 0:\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "    \n",
    "    if not boxes:\n",
    "        boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "        labels = torch.zeros((0,), dtype=torch.int64)\n",
    "    else:\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32).view(-1, 4)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "    masks = torch.empty(len(labels), 1, 1, dtype=torch.uint8)\n",
    "    return {'boxes': boxes, 'labels': labels, 'masks': masks}\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "        targets = [process_target(t) for t in targets]\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with amp.autocast():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass with scaler\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(losses).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {losses.item():.4f}\")\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.4f}\\n\")\n",
    "\n",
    "    # --- SAVE CHECKPOINT (Every Epoch) ---\n",
    "    checkpoint_data = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "        'loss': avg_epoch_loss\n",
    "    }\n",
    "    torch.save(checkpoint_data, checkpoint_path)\n",
    "    print(f\"\\n\\nCheckpoint saved for Epoch {epoch+1}\\n\\n\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Saving Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0G8JtWO97hG"
   },
   "outputs": [],
   "source": [
    "# Save the Trained Model\n",
    "model_save_path = \"/home/somel/code/FYP_Project/models/fast_rcnn/fast_rcnn_model_2.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Evaluation with mAP\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "def evaluate_model_mAP(model, data_loader, coco_gt, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    \n",
    "    print(\"Generating predictions...\")\n",
    "    with torch.no_grad():\n",
    "        # Use enumerate to track the batch index\n",
    "        for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                # Calculate the global index in the dataset\n",
    "                # This works because shuffle=False for the validation loader\n",
    "                dataset_index = batch_idx * data_loader.batch_size + i\n",
    "                \n",
    "                # Retrieve the correct image_id from the dataset\n",
    "                image_id = data_loader.dataset.ids[dataset_index]\n",
    "                \n",
    "                # Ensure boxes, labels, scores are on CPU and converted to numpy\n",
    "                boxes = output['boxes'].cpu().numpy()\n",
    "                labels = output['labels'].cpu().numpy()\n",
    "                scores = output['scores'].cpu().numpy()\n",
    "\n",
    "                for j in range(len(boxes)):\n",
    "                    # COCO format requires [x, y, w, h]\n",
    "                    x1, y1, x2, y2 = boxes[j]\n",
    "                    w = x2 - x1\n",
    "                    h = y2 - y1\n",
    "                    bbox = [float(x1), float(y1), float(w), float(h)]\n",
    "\n",
    "                    all_predictions.append({\n",
    "                        'image_id': int(image_id),\n",
    "                        'category_id': int(labels[j]),\n",
    "                        'bbox': bbox,\n",
    "                        'score': float(scores[j])\n",
    "                    })\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Processed batch {batch_idx}\")\n",
    "\n",
    "    if not all_predictions:\n",
    "        print(\"No predictions generated.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Total predictions: {len(all_predictions)}\")\n",
    "\n",
    "    # Load detections into COCO format\n",
    "    coco_dt = coco_gt.loadRes(all_predictions)\n",
    "\n",
    "    # Initialize COCOeval\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "    coco_eval.params.imgIds = data_loader.dataset.ids\n",
    "    \n",
    "    print(\"Running evaluation...\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    return coco_eval\n",
    "\n",
    "# Initialize COCO ground truth\n",
    "# val_annotations_file is defined in previous cells\n",
    "coco_gt = COCO(val_annotations_file)\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_model_mAP(model, val_data_loader, coco_gt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcjAG7Lo98f1"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# Load the saved model for inference\n",
    "if model_save_path:\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Example: Run inference on a validation image\n",
    "with torch.no_grad():\n",
    "    # Get a sample from validation set\n",
    "    img, target = val_dataset[0]\n",
    "    img = img.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Run model\n",
    "    predictions = model(img)\n",
    "\n",
    "    # Print predictions\n",
    "    print(\"Predictions for sample image:\")\n",
    "    print(f\"Boxes: {predictions[0]['boxes']}\")\n",
    "    print(f\"Labels: {predictions[0]['labels']}\")\n",
    "    print(f\"Scores: {predictions[0]['scores']}\")\n",
    "\n",
    "# For full evaluation, you could loop over val_data_loader and compute mAP, etc.\n",
    "# But that requires additional libraries like pycocotools for COCO metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "'''\n",
    "NOTE:\n",
    "ID\n",
    "1 : Groupers\n",
    "2 : Seabass\n",
    "3 : Tilapia\n",
    "\n",
    "'''\n",
    "def view_prediction(img_tensor, prediction, threshold=0.5):\n",
    "    # Handle batch dimension if present (1, C, H, W) -> (C, H, W)\n",
    "    if img_tensor.dim() == 4:\n",
    "        img_tensor = img_tensor.squeeze(0)\n",
    "    \n",
    "    # Convert tensor image back to numpy for plotting\n",
    "    # Move to cpu and rearrange dimensions from (C, H, W) to (H, W, C)\n",
    "    img_np = img_tensor.cpu().permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(img_np)\n",
    "\n",
    "    # Get boxes, scores, and labels from the first prediction in the batch\n",
    "    boxes = prediction[0]['boxes'].cpu().detach().numpy()\n",
    "    scores = prediction[0]['scores'].cpu().detach().numpy()\n",
    "    labels = prediction[0]['labels'].cpu().detach().numpy()\n",
    "\n",
    "    for i, box in enumerate(boxes):\n",
    "        if scores[i] > threshold:\n",
    "            # Create a Rectangle patch\n",
    "            # box format: [x_min, y_min, x_max, y_max]\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "            \n",
    "            rect = patches.Rectangle(\n",
    "                (x_min, y_min), width, height, \n",
    "                linewidth=2, edgecolor='r', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(\n",
    "                x_min, y_min, \n",
    "                f\"Class: {labels[i]} ({scores[i]:.2f})\", \n",
    "                color='white', fontsize=10, backgroundcolor='red'\n",
    "            )\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# --- Run Inference on a Random Sample ---\n",
    "# 1. Pick a random image from validation set\n",
    "idx = random.randint(0, len(val_dataset) - 1)\n",
    "img, _ = val_dataset[idx]\n",
    "\n",
    "# 2. Prepare image for model (add batch dimension)\n",
    "img_input = img.unsqueeze(0).to(device)\n",
    "\n",
    "# 3. Get predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(img_input)\n",
    "\n",
    "# 4. Visualize\n",
    "print(f\"Visualizing prediction for image index {idx}\")\n",
    "view_prediction(img, predictions, threshold=0.3) # Lowered threshold slightly to see more"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
