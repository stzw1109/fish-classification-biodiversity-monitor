{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7705a523",
   "metadata": {},
   "source": [
    "## Part 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f05ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f679f",
   "metadata": {},
   "source": [
    "## Part 2: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b0506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset directories and files...\n",
      "✓ /workspaces/ai-projects/Dataset/Fish_COCO/train exists\n",
      "✓ /workspaces/ai-projects/Dataset/Fish_COCO/valid exists\n",
      "✓ /workspaces/ai-projects/Dataset/Fish_COCO/train/_annotations.coco.json exists\n",
      "✓ /workspaces/ai-projects/Dataset/Fish_COCO/valid/_annotations.coco.json exists\n",
      "Number of training images: 2925\n",
      "Number of validation images: 122\n"
     ]
    }
   ],
   "source": [
    "# Dataset Preparation\n",
    "# Set the root directory for your dataset\n",
    "root_dir = \"/workspaces/ai-projects/Dataset/Fish_COCO\"  # Change this to your dataset path if different\n",
    "\n",
    "# For Fast R-CNN training with COCO format, you need:\n",
    "# - A folder with images (e.g., 'images/')\n",
    "# - An annotations.json file with COCO-style annotations (including bounding boxes)\n",
    "# Boxes are [xmin, ymin, xmax, ymax] in pixel coordinates, labels are class IDs\n",
    "\n",
    "# Roboflow's COCO export provides this: images/ and annotations.json with bounding boxes.\n",
    "# No separate annotation files needed—PyTorch handles the parsing.\n",
    "\n",
    "# If your data is in subfolders like Bass/, Groupers/, etc., you may need to:\n",
    "# 1. Collect all images into one 'images/' folder\n",
    "# 2. Ensure annotations.json references the correct image filenames\n",
    "# 3. Split into train/val/test folders\n",
    "\n",
    "# For now, assume you have prepared the data in the following structure:\n",
    "# root_dir/\n",
    "#   train/\n",
    "#     images/\n",
    "#     annotations.json\n",
    "#   val/\n",
    "#     images/\n",
    "#     annotations.json\n",
    "\n",
    "train_images_dir = os.path.join(root_dir, \"train\")\n",
    "train_annotations_file = os.path.join(root_dir, \"train\", \"_annotations.coco.json\")\n",
    "val_images_dir = os.path.join(root_dir, \"valid\")\n",
    "val_annotations_file = os.path.join(root_dir, \"valid\", \"_annotations.coco.json\")\n",
    "\n",
    "# Check if directories and files exist\n",
    "print(\"Checking dataset directories and files...\")\n",
    "for dir_path in [train_images_dir, val_images_dir]:\n",
    "    if os.path.exists(dir_path):\n",
    "        print(f\"✓ {dir_path} exists\")\n",
    "    else:\n",
    "        print(f\"✗ {dir_path} does not exist - please create and populate it\")\n",
    "\n",
    "for file_path in [train_annotations_file, val_annotations_file]:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"✓ {file_path} exists\")\n",
    "    else:\n",
    "        print(f\"✗ {file_path} does not exist - please export from Roboflow\")\n",
    "\n",
    "# List number of files (optional check)\n",
    "if os.path.exists(train_images_dir):\n",
    "    num_train_images = len([f for f in os.listdir(train_images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    print(f\"Number of training images: {num_train_images}\")\n",
    "if os.path.exists(val_images_dir):\n",
    "    num_val_images = len([f for f in os.listdir(val_images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    print(f\"Number of validation images: {num_val_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a62221",
   "metadata": {},
   "source": [
    "## Part 3: Defining Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Dataset Class\n",
    "# We'll use torchvision's CocoDetection, which loads COCO-format data (images + annotations.json)\n",
    "# It automatically parses the JSON for bounding boxes and labels for Fast R-CNN\n",
    "# No custom class needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b970343",
   "metadata": {},
   "source": [
    "## Part 4: Define Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d18e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Transforms\n",
    "def get_transforms(train):\n",
    "    \"\"\"\n",
    "    Define transforms for training and validation.\n",
    "    For now, no augmentations are applied.\n",
    "    ToTensor is handled in the dataset using F.to_tensor.\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    if train:\n",
    "        # Add training augmentations here if needed, e.g.:\n",
    "        # transforms.append(RandomHorizontalFlip(0.5))\n",
    "        # transforms.append(RandomCrop(...))\n",
    "        pass\n",
    "    # Note: Normalization or resizing can be added here if required\n",
    "    return Compose(transforms) if transforms else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14986e73",
   "metadata": {},
   "source": [
    "## Part 5: Load and Modify the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Modify the Pre-trained Fast R-CNN Model\n",
    "# Load pre-trained Fast R-CNN with ResNet-50 backbone\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Number of classes: background + number of fish classes\n",
    "# For your project, if you have multiple species (e.g., Bass, Tilapia), set accordingly\n",
    "# For now, assuming 2 fish classes + background = 3\n",
    "num_classes = 3  # Adjust based on your classes\n",
    "\n",
    "# Modify the box predictor head\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "print(f\"Model loaded with {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a97f895",
   "metadata": {},
   "source": [
    "## Part 6: Setting up Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Data Loaders\n",
    "# Create training and validation datasets using CocoDetection\n",
    "# CocoDetection(root, annFile, transforms=None) - root is images folder, annFile is annotations.json\n",
    "train_dataset = CocoDetection(root=train_images_dir, annFile=train_annotations_file, transforms=get_transforms(train=True))\n",
    "val_dataset = CocoDetection(root=val_images_dir, annFile=val_annotations_file, transforms=get_transforms(train=False))\n",
    "\n",
    "# Create data loaders\n",
    "# Batch size: adjust based on your GPU memory (start small, e.g., 2)\n",
    "batch_size = 2\n",
    "# num_workers: set to 0 if you encounter issues, or match your CPU cores\n",
    "num_workers = 4\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=lambda x: tuple(zip(*x))  # Custom collate for variable-sized targets\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627ce72",
   "metadata": {},
   "source": [
    "## Part 7: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10  # Adjust as needed\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "        # Move images and targets to device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "        # Optional: print batch loss\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {losses.item():.4f}\")\n",
    "\n",
    "    # Average loss per epoch\n",
    "    avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cca273",
   "metadata": {},
   "source": [
    "## Part 8: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec00ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Trained Model\n",
    "model_save_path = \"fast_rcnn_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffaf526",
   "metadata": {},
   "source": [
    "## Part 9: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075af918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# Load the saved model for inference\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()\n",
    "\n",
    "# Example: Run inference on a validation image\n",
    "with torch.no_grad():\n",
    "    # Get a sample from validation set\n",
    "    img, target = val_dataset[0]\n",
    "    img = img.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Run model\n",
    "    predictions = model(img)\n",
    "\n",
    "    # Print predictions\n",
    "    print(\"Predictions for sample image:\")\n",
    "    print(f\"Boxes: {predictions[0]['boxes']}\")\n",
    "    print(f\"Labels: {predictions[0]['labels']}\")\n",
    "    print(f\"Scores: {predictions[0]['scores']}\")\n",
    "\n",
    "# For full evaluation, you could loop over val_data_loader and compute mAP, etc.\n",
    "# But that requires additional libraries like pycocotools for COCO metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
