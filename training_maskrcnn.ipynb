{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d07c9c42",
   "metadata": {},
   "source": [
    "## Part 1: Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a08c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "# Set PyTorch CUDA memory configuration\n",
    "# helps with reducing GPU memory fragmentation, therefore preventing out-of-memory errors\n",
    "os.environ['PYTORCH_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e422a34",
   "metadata": {},
   "source": [
    "## Part 2: Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e18f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation\n",
    "# Set the root directory for your dataset\n",
    "root_dir = \"/workspaces/ai-projects/Dataset/Fish_COCO\"  # Change this to your dataset path if different\n",
    "\n",
    "train_images_dir = os.path.join(root_dir, \"train\")\n",
    "train_annotations_file = os.path.join(root_dir, \"train\", \"_annotations.coco.json\")\n",
    "val_images_dir = os.path.join(root_dir, \"valid\")\n",
    "val_annotations_file = os.path.join(root_dir, \"valid\", \"_annotations.coco.json\")\n",
    "\n",
    "# Check if directories and files exist\n",
    "print(\"Checking dataset directories and files...\")\n",
    "for dir_path in [train_images_dir, val_images_dir]:\n",
    "    if os.path.exists(dir_path):\n",
    "        print(f\"✓ {dir_path} exists\")\n",
    "    else:\n",
    "        print(f\"✗ {dir_path} does not exist - please create and populate it\")\n",
    "\n",
    "for file_path in [train_annotations_file, val_annotations_file]:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"✓ {file_path} exists\")\n",
    "    else:\n",
    "        print(f\"✗ {file_path} does not exist - please export from Roboflow\")\n",
    "\n",
    "# List number of files (optional check)\n",
    "if os.path.exists(train_images_dir):\n",
    "    num_train_images = len([f for f in os.listdir(train_images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    print(f\"Number of training images: {num_train_images}\")\n",
    "if os.path.exists(val_images_dir):\n",
    "    num_val_images = len([f for f in os.listdir(val_images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    print(f\"Number of validation images: {num_val_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aafd814",
   "metadata": {},
   "source": [
    "## Part 3: Defining Custom Dataset Class (Obsolete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Custom Dataset Class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    NOTE:\n",
    "    for now since im using the annotations theres no need for the custom dataset class.\n",
    "    '''\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # List and sort image files\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        # List and sort mask files\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"masks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = F.to_tensor(img)\n",
    "\n",
    "        # Load mask\n",
    "        mask_path = os.path.join(self.root, \"masks\", self.masks[idx])\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = torch.as_tensor(np.array(mask), dtype=torch.uint8)\n",
    "\n",
    "        # Get unique object IDs (assuming 0 is background)\n",
    "        obj_ids = torch.unique(mask)\n",
    "        obj_ids = obj_ids[obj_ids != 0]  # Remove background\n",
    "\n",
    "        # Create binary masks for each object\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # Compute bounding boxes from masks\n",
    "        boxes = []\n",
    "        for i in range(len(obj_ids)):\n",
    "            pos = torch.where(masks[i])\n",
    "            xmin = torch.min(pos[1])\n",
    "            xmax = torch.max(pos[1])\n",
    "            ymin = torch.min(pos[0])\n",
    "            ymax = torch.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # Labels: assuming all objects are the same class (fish), label=1\n",
    "        # For multiple classes, you'd need to encode class info in masks or separate files\n",
    "        labels = torch.ones((len(obj_ids),), dtype=torch.int64)\n",
    "\n",
    "        # Convert masks to uint8\n",
    "        masks = masks.to(torch.uint8)\n",
    "\n",
    "        # Target dictionary for Mask R-CNN\n",
    "        target = {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ade960",
   "metadata": {},
   "source": [
    "## Part 4: Define Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Transforms\n",
    "def get_transforms(train):\n",
    "    \"\"\"\n",
    "    Define transforms for training and validation.\n",
    "    For now, no augmentations are applied.\n",
    "    ToTensor is handled in the dataset using F.to_tensor.\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    if train:\n",
    "        # Add training augmentations here if needed, e.g.:\n",
    "        # transforms.append(RandomHorizontalFlip(0.5))\n",
    "        # transforms.append(RandomCrop(...))\n",
    "        pass\n",
    "    # Note: Normalization or resizing can be added here if required\n",
    "    return Compose(transforms) if transforms else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7ca059",
   "metadata": {},
   "source": [
    "## Part 5: Load and Modify the Pre-trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b91e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Modify the Pre-trained Mask R-CNN Model\n",
    "# Load pre-trained Mask R-CNN with ResNet-50 backbone\n",
    "# pre-trained is set to True so that COCO dataset training does not have to start from scratch\n",
    "model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Number of classes: background + number of fish classes\n",
    "# For your project, if you have multiple species (e.g., Bass, Tilapia), set accordingly\n",
    "# For now, assuming 2 fish classes + background = 3\n",
    "num_classes = 4  # Adjust based on your classes\n",
    "\n",
    "# Modify the box predictor head\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Modify the mask predictor head\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256  # Standard value\n",
    "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "print(f\"Model loaded with {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f49bf5",
   "metadata": {},
   "source": [
    "## Part 6: Setting up Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a460a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Data Loaders\n",
    "# Create training and validation datasets using CocoDetection\n",
    "# CocoDetection(root, annFile, transforms=None) - root is images folder, annFile is annotations.json\n",
    "train_dataset = CocoDetection(root=train_images_dir, annFile=train_annotations_file, transforms=get_transforms(train=True))\n",
    "val_dataset = CocoDetection(root=val_images_dir, annFile=val_annotations_file, transforms=get_transforms(train=False))\n",
    "\n",
    "# Create data loaders\n",
    "# Batch size: reduce for GPU memory (try 2-4 for Mask R-CNN)\n",
    "batch_size = 4\n",
    "# num_workers: reduce to avoid memory issues\n",
    "num_workers = 2\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=lambda x: tuple(zip(*x))  # Custom collate for variable-sized targets\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075da63",
   "metadata": {},
   "source": [
    "## Part 7: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8cd675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50  # Adjust as needed\n",
    "\n",
    "# Function to process COCO target from list of annotations to dict\n",
    "def process_target(target):\n",
    "    if isinstance(target, dict):\n",
    "        return target\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    masks = []\n",
    "    for ann in target:\n",
    "        bbox = ann['bbox']\n",
    "        boxes.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
    "        labels.append(ann['category_id'])\n",
    "        # For masks, if segmentation is present, you can add conversion here\n",
    "        # For now, use placeholder\n",
    "    boxes = torch.tensor(boxes, dtype=torch.float32).view(-1, 4)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    masks = torch.empty(len(labels), 1, 1, dtype=torch.uint8)  # placeholder\n",
    "    return {'boxes': boxes, 'labels': labels, 'masks': masks}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "        # Process targets\n",
    "        targets = [process_target(t) for t in targets]\n",
    "        # Convert PIL images to tensors and move to device\n",
    "        images = [F.to_tensor(img).to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with amp.autocast():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass with scaler\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(losses).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "        # Optional: print batch loss\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {losses.item():.4f}\")\n",
    "\n",
    "    # Average loss per epoch\n",
    "    avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c877c78",
   "metadata": {},
   "source": [
    "## Part 8: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6622fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Trained Model\n",
    "model_save_path = \"mask_rcnn_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b54cae",
   "metadata": {},
   "source": [
    "## Part 9: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# Load the saved model for inference\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()\n",
    "\n",
    "# Example: Run inference on a validation image\n",
    "with torch.no_grad():\n",
    "    # Get a sample from validation set\n",
    "    img, target = val_dataset[0]\n",
    "    target = process_target(target)\n",
    "    img = F.to_tensor(img).unsqueeze(0).to(device)  # Convert to tensor and add batch dimension\n",
    "\n",
    "    # Run model\n",
    "    predictions = model(img)\n",
    "\n",
    "    # Print predictions\n",
    "    print(\"Predictions for sample image:\")\n",
    "    print(f\"Boxes: {predictions[0]['boxes']}\")\n",
    "    print(f\"Labels: {predictions[0]['labels']}\")\n",
    "    print(f\"Scores: {predictions[0]['scores']}\")\n",
    "    print(f\"Masks shape: {predictions[0]['masks'].shape}\")\n",
    "\n",
    "# For full evaluation, you could loop over val_data_loader and compute mAP, etc.\n",
    "# But that requires additional libraries like pycocotools for COCO metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed52c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze(0).permute(1, 2, 0).cpu())  # Convert tensor back to image\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
